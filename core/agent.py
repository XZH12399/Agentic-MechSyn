import json
import numpy as np
import traceback
import networkx as nx  # âœ¨ æ–°å¢: ç”¨äºæ‹“æ‰‘è·¯å¾„è®¡ç®—
from openai import OpenAI

# === 1. å¯¼å…¥ Prompt æ¨¡æ¿ ===
from .prompt_templates import (
    TOPOLOGY_GEN_SYSTEM_PROMPT,
    TOOL_SELECTION_SYSTEM_PROMPT,
    REFLECTION_SYSTEM_PROMPT,  # âœ¨ æ–°å¢
    STRUCTURED_OUTPUT_SUFFIX
)

# === 2. å¯¼å…¥ Schema ===
from .schemas import (
    TaskTemplate,
    TopologyResponse,
    ToolSelectionResponse,
    ReflectionResponse,  # âœ¨ æ–°å¢
    ReflectionAction  # âœ¨ æ–°å¢
)

# === 3. å¯¼å…¥æ ¸å¿ƒé€»è¾‘æ¨¡å— ===
from .task_parser import TaskParser
from .memory import ExperienceManager

# === 4. å¯¼å…¥å·¥å…·æ¨¡å— ===
from tools.tool_registry import AVAILABLE_TOOLS_DEF
from utils.tensor_adapter import MechanismTensorAdapter
from tools.optimizer_engine import MechanismOptimizer
from tools.evaluator_engine import MechanismEvaluator


class MechanismAgent:
    def __init__(self, cfg):
        self.cfg = cfg
        self.client = OpenAI(api_key=cfg.api_key, base_url=cfg.base_url)
        self.model = cfg.model_name

        self.parser = TaskParser(self.client, self.model)
        self.memory = ExperienceManager()
        self.optimizer = MechanismOptimizer(cfg.physics)
        self.evaluator = MechanismEvaluator()

        # âœ¨ å…¨å±€è½¨è¿¹è®°å½•å™¨
        self.trace_log = []

    def _record(self, tag, content):
        """è¾…åŠ©å‡½æ•°: è®°å½•è½¨è¿¹åˆ° trace_log"""
        entry = f"\n[{tag}]\n{content}\n" + "-" * 40
        self.trace_log.append(entry)

    def run_pipeline(self, user_input):
        # æ¸…ç©ºä¸Šä¸€è½®æ—¥å¿—
        self.trace_log = []
        print(f"\nğŸš€ [Start] å¯åŠ¨æœºæ„ç»¼åˆæµæ°´çº¿ (å¸¦è‡ªæˆ‘åæ€å¾ªç¯)...")
        self._record("User Input", user_input)

        # 1. ä»»åŠ¡è§£æ (åªåšä¸€æ¬¡)
        task_template = self._step1_parse_task(user_input)
        self._record("Step 1 Task Template", json.dumps(task_template, indent=2, ensure_ascii=False))

        # 1.5 ç»éªŒæ£€ç´¢
        exp_context = self._step1_5_retrieve_experience(task_template)

        # === è¿›å…¥è¿­ä»£å¾ªç¯ ===
        max_retries = self.cfg.agent.get('max_turns', 5)
        current_topology_response = None  # ä¿å­˜å½“å‰çš„æ‹“æ‰‘å¯¹è±¡
        tensor_data = None

        # åˆå§‹ç”Ÿæˆ (Step 2)
        current_topology_response, tensor_data = self._step2_generate_topology(task_template, exp_context)

        best_report = {"final_score": -1}
        # best_design = None

        for attempt in range(max_retries):
            print(f"\nğŸ”„ [Iteration {attempt + 1}/{max_retries}] å¼€å§‹æ–°ä¸€è½®å°è¯•...")

            # 3. å·¥å…·é€‰æ‹© (Step 3)
            # å› ä¸ºæ‹“æ‰‘å¯èƒ½åœ¨åæ€ä¸­æ”¹å˜ï¼Œæ‰€ä»¥æ¯æ¬¡éƒ½è¦é‡æ–°é€‰æ‹©
            topology_dict = current_topology_response.topology.model_dump()
            tools_config = self._step3_select_tools(task_template, topology_dict)

            # 4. ä¼˜åŒ– (Step 4)
            # æ³¨æ„: è¿™é‡Œçš„ task_template å¯èƒ½å·²ç»è¢« _generate_and_inject_path ä¿®æ”¹è¿‡ (æ³¨å…¥äº†è·¯å¾„)
            optimized_geometry, opt_log_str = self._step4_optimize(tensor_data, task_template, tools_config)

            # 5. è¯„ä¼° (Step 5)
            report = self._step5_evaluate(optimized_geometry, tools_config)
            print(f"ğŸ“‰ [Result] å½“å‰å¾—åˆ†: {report['final_score']:.2f}")
            self._record(f"Step 5 Evaluation (Attempt {attempt + 1})", json.dumps(report, indent=2))

            # è®°å½•æœ€ä½³ç»“æœ
            if report['final_score'] > best_report['final_score']:
                best_report = report
                # best_design = optimized_geometry

            # æˆåŠŸåˆ¤å®š (ä¾‹å¦‚ >= 90 åˆ†)
            if report['final_score'] >= 90.0:
                print("ğŸ‰ [Success] è¾¾åˆ°ç›®æ ‡åˆ†æ•°ï¼Œåœæ­¢è¿­ä»£ï¼")
                break

            # === 6. è‡ªæˆ‘åæ€ (Step Reflect) ===
            # å¦‚æœæ²¡è¾¾åˆ°æ»¡åˆ†ï¼Œä¸”è¿˜æœ‰å‰©ä½™æ¬¡æ•°ï¼Œè¿›è¡Œåæ€
            if attempt < max_retries - 1:
                reflection = self._step_reflect(task_template, current_topology_response, report)

                # === 7. æ‰§è¡Œä¿®æ­£ç­–ç•¥ ===
                if reflection.action == ReflectionAction.KEEP_CURRENT:
                    print("ğŸ¤” [Reflect] Agent è®¤ä¸ºå½“å‰ç»“æœå·²è¶³å¤Ÿå¥½ã€‚")
                    break

                elif reflection.action == ReflectionAction.REINIT_GEOMETRY:
                    print("ğŸ² [Reflect] ç­–ç•¥: é‡æ–°åˆå§‹åŒ–å‡ ä½•å‚æ•° (Re-Init Tensor)")
                    # é‡æ–°ç”Ÿæˆå¼ é‡ (Adapter ä¼šéšæœºåˆå§‹åŒ–å‚æ•°)
                    # æ³¨æ„ï¼šä¿ç•™æ‹“æ‰‘ç»“æ„ï¼Œåªé‡ç½®æ•°å€¼
                    tensor_data = self._convert_and_print_tensor(topology_dict)
                    # è¿™é‡Œçš„ tensor_data å·²ç»æ˜¯å…¨æ–°çš„éšæœºåˆå€¼äº†

                elif reflection.action == ReflectionAction.RESELECT_ANCHORS:
                    print(
                        f"âš“ [Reflect] ç­–ç•¥: é‡æ–°é€‰æ‹©åŸºåº§/æœ«ç«¯ -> Ground: {reflection.suggested_ground_nodes}, EE: {reflection.suggested_ee_node}")
                    # ä¿®æ”¹å…ƒæ•°æ®
                    if reflection.suggested_ground_nodes:
                        current_topology_response.meta.ground_nodes = reflection.suggested_ground_nodes
                    if reflection.suggested_ee_node:
                        current_topology_response.meta.ee_node = reflection.suggested_ee_node

                    # âœ¨ é‡è¦: ä¿®æ”¹äº† Ground/EE åï¼Œå¿…é¡»é‡æ–°ç”Ÿæˆè·¯å¾„ï¼
                    self._generate_and_inject_path(current_topology_response, task_template)
                    # å¼ é‡ç»“æ„ä¸å˜ï¼Œä½† task_template é‡Œçš„ Path å˜äº†

                elif reflection.action == ReflectionAction.REGENERATE_TOPOLOGY:
                    print(f"ğŸ¨ [Reflect] ç­–ç•¥: æ‹“æ‰‘é‡ç»˜ (Regenerate)")
                    # å°†å»ºè®®åŠ å…¥åˆ° prompt ä¸­
                    suggestion = reflection.topology_suggestion or "å°è¯•ä¸åŒçš„ç»“æ„"
                    refined_context = f"{exp_context}\n[ä¸Šä¸€è½®å¤±è´¥æ•™è®­]: {suggestion}"
                    # é‡æ–°æ‰§è¡Œ Step 2 (LLM ç”Ÿæˆ -> è·¯å¾„æ³¨å…¥ -> è½¬å¼ é‡)
                    current_topology_response, tensor_data = self._step2_generate_topology(task_template,
                                                                                           refined_context)

        # å¾ªç¯ç»“æŸï¼Œå­˜å…¥ç»éªŒæ± 
        self._step6_store_experience(user_input, task_template, tensor_data, best_report)
        return best_report

    # =========================================================================
    #                               Step Functions
    # =========================================================================

    def _step1_parse_task(self, user_input):
        print(f"\n--- Step 1: ç”Ÿæˆä»»åŠ¡æ¨¡æ¿ ---")
        return self.parser.parse(user_input)

    def _step1_5_retrieve_experience(self, task_template):
        print(f"\n--- Step 1.5: æ£€ç´¢è¿‡å¾€ç»éªŒ ---")
        past_exps = self.memory.retrieve_relevant(task_template)
        exp_context = ""
        if past_exps:
            print(f"ğŸ“š å‘ç° {len(past_exps)} æ¡ç›¸å…³ç»éªŒ...")
            exp_context = f"\nå‚è€ƒè¿‡å¾€æˆåŠŸæ¡ˆä¾‹:\n{json.dumps(past_exps[0]['report'], indent=2)}"
        return exp_context

    def _step2_generate_topology(self, task_template, exp_context):
        print(f"\n--- Step 2: ç”Ÿæˆåˆå§‹æ‹“æ‰‘è‰å›¾ (DeepSeek Compat) ---")

        # 1. LLM ç”Ÿæˆä¸è§£æ
        response_obj = self._call_topology_llm(task_template, exp_context)

        # 2. âœ¨ å…¨è‡ªåŠ¨ç”Ÿæˆè·¯å¾„å¹¶æ³¨å…¥åˆ° Task æ¨¡æ¿ä¸­
        #    æ³¨æ„ï¼šè¿™é‡Œä¸å†æ“ä½œ response_obj.metaï¼Œè€Œæ˜¯ç›´æ¥ä¿®æ”¹ task_template
        self._generate_and_inject_path(response_obj, task_template)

        # 3. è½¬æ¢ä¸ºç‰©ç†å¼ é‡å¹¶æ‰“å°
        topology_data = response_obj.topology.model_dump()
        tensor_data = self._convert_and_print_tensor(topology_data)

        return response_obj, tensor_data

    # === Step 2 è¾…åŠ©å‡½æ•° ===

    def _call_topology_llm(self, task_template, exp_context):
        """è¾…åŠ©å‡½æ•°: è´Ÿè´£ Prompt æ„å»ºã€LLM è°ƒç”¨ä¸åŸºç¡€æ ¡éªŒ"""
        try:
            # 1. å‡†å¤‡ Schema å’Œ Prompt
            schema_str = json.dumps(TopologyResponse.model_json_schema(), indent=2, ensure_ascii=False)
            full_system_prompt = TOPOLOGY_GEN_SYSTEM_PROMPT + STRUCTURED_OUTPUT_SUFFIX.format(schema=schema_str)
            user_content = f"ä»»åŠ¡æ¨¡æ¿:\n{json.dumps(task_template)}\n{exp_context}\nè¯·ç”Ÿæˆåˆå§‹æ‹“æ‰‘ã€‚"

            self._record("Step 2 LLM Input (Prompt)", f"System:\n{full_system_prompt}\n\nUser:\n{user_content}")

            # 2. è°ƒç”¨ API
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": full_system_prompt},
                    {"role": "user", "content": user_content}
                ],
                response_format={"type": "json_object"}
            )

            # 3. è§£æä¸æ ¡éªŒ
            content = response.choices[0].message.content
            print(f"\nğŸ” [Debug] LLM åŸå§‹è¾“å‡º (Raw JSON):\n{content}")
            self._record("Step 2 LLM Output (Raw)", content)

            response_obj = TopologyResponse.model_validate_json(content)
            print(f"ğŸ§  [Thought]: {response_obj.thought_trace[:150]}...")

            return response_obj

        except Exception as e:
            print(f"âŒ [Topology LLM Error] ç”Ÿæˆå¤±è´¥: {e}")
            raise e

    def _generate_and_inject_path(self, response_obj, task_template):
        """
        è¾…åŠ©å‡½æ•°: æ ¹æ®æ‹“æ‰‘å›¾è‡ªåŠ¨è®¡ç®—è·¯å¾„åºåˆ—ï¼Œå¹¶æ³¨å…¥åˆ° task_template ä¸­ã€‚
        ä¸å†ä¾èµ– response_obj.meta.path_sequenceã€‚
        """
        try:
            topology_data = response_obj.topology.model_dump()

            # 1. æ„å»ºå›¾
            G = nx.Graph()
            for conn in topology_data['connections']:
                G.add_edge(int(conn['source']), int(conn['target']))

            # 2. è·å–ç«¯ç‚¹ (LLM åªè´Ÿè´£å®šè¿™ä¸¤ä¸ªç‚¹)
            if not response_obj.meta.ground_nodes:
                print("âš ï¸ [Path Gen] æœªå®šä¹‰ ground_nodesï¼Œæ— æ³•ç”Ÿæˆè·¯å¾„ã€‚")
                return

            start_node = int(response_obj.meta.ground_nodes[0])
            end_node = int(response_obj.meta.ee_node)

            print(f"ğŸ› ï¸ [Path Gen] æ­£åœ¨è®¡ç®—è¿åŠ¨é“¾: {start_node} -> ... -> {end_node}")

            # 3. è®¡ç®—æœ€çŸ­ç‰©ç†è·¯å¾„
            shortest_chain = nx.shortest_path(G, source=start_node, target=end_node)

            # 4. å¯»æ‰¾ Ghost Nodes (è¾…åŠ©èŠ‚ç‚¹)
            def get_ghost(current, exclude_list):
                neighbors = list(G.neighbors(current))
                for n in neighbors:
                    if n not in exclude_list: return n
                # å¦‚æœæ²¡æœ‰é¢å¤–é‚»å±…ï¼Œå›é€€å–è‡ªèº«æˆ–é“¾ä¸Šçš„é‚»å±…ï¼ˆä»…åšé˜²å´©å¤„ç†ï¼‰
                return neighbors[0] if neighbors else current

            ghost_in = get_ghost(start_node, shortest_chain)
            ghost_out = get_ghost(end_node, shortest_chain)

            # 5. ç»„è£…å®Œæ•´è·¯å¾„
            full_path = [ghost_in] + shortest_chain + [ghost_out]

            msg = f"Start: {start_node}, End: {end_node}\nGenerated Path: {full_path}"
            print(f"    - âœ… è‡ªåŠ¨ç”Ÿæˆè·¯å¾„: {full_path}")
            self._record("Step 2 Auto-Fix (Path Gen)", msg)

            # 6. âœ¨ æ ¸å¿ƒæ­¥éª¤: æ³¨å…¥åˆ° Task Template
            if 'targets' not in task_template:
                task_template['targets'] = {}

            task_template['targets']['target_path_sequence'] = full_path

        except Exception as e:
            print(f"âŒ [Path Gen Error] è·¯å¾„ç”Ÿæˆå¤±è´¥: {e}")
            traceback.print_exc()
            if 'targets' in task_template:
                task_template['targets'].pop('target_path_sequence', None)

    def _convert_and_print_tensor(self, topology_data):
        """è¾…åŠ©å‡½æ•°: è´Ÿè´£å¼ é‡è½¬æ¢åŠè¯¦ç»†æ—¥å¿—"""
        try:
            # 1. åŠ¨æ€å†³å®š num_nodes
            node_ids = [int(nid) for nid in topology_data['nodes'].keys()]
            if node_ids:
                max_id = max(node_ids)
                calculated_num = max_id + 1
                num_nodes = max(calculated_num, 4)
                print(f"ğŸ§© [Topology] èŠ‚ç‚¹è§„æ¨¡: {calculated_num} (Tensor Size: {num_nodes}x{num_nodes})")
            else:
                print("âš ï¸ [Warning] æœªæ£€æµ‹åˆ°èŠ‚ç‚¹ï¼Œä½¿ç”¨é»˜è®¤å€¼ 8")
                num_nodes = 8

            # 2. è½¬åŒ–å¼ é‡
            adapter = MechanismTensorAdapter(num_nodes=num_nodes)
            tensor_data = adapter.json_to_tensor(topology_data)

            print(f"ğŸ“Š [Tensor] å¼ é‡å½¢çŠ¶: {tensor_data.shape}")

            # 3. è¯¦ç»†æ‰“å°
            with np.printoptions(threshold=np.inf, linewidth=200, precision=4, suppress=True):
                if np.all(tensor_data[0] == 0):
                    print("âš ï¸ [Warning] å¼ é‡å…¨ä¸º 0ï¼")
                else:
                    self._print_connection_details(tensor_data)

            return tensor_data

        except Exception as e:
            print(f"âŒ [Tensor Error] è½¬æ¢å¤±è´¥: {e}")
            traceback.print_exc()
            return np.zeros((5, 8, 8))

    def _print_connection_details(self, tensor_data):
        """è¾…åŠ©å‡½æ•°ï¼šæ‰“å°è¿æ¥è¡¨ (ä¿®æ­£ç‰ˆ: æ˜¾å¼æ‰“å°åå‘èŠ‚ç‚¹ç±»å‹)"""
        rows, cols = np.where(tensor_data[0] > 0)

        print(f"âœ… [Success] è§£æéé›¶è¿æ¥æ•°: {len(rows) // 2}")
        print("\nğŸ” [Debug] éé›¶è¿æ¥è¯¦æƒ…:")
        print(f"{'Link':<10} | {'Type':<5} | {'a (mm)':<10} | {'alpha':<10} | {'offset':<15}")
        print("-" * 65)

        for r, c in zip(rows, cols):
            if r < c:
                # 1. æ­£å‘ r -> c
                type_val_r = tensor_data[1, r, c]
                type_str_r = "R" if type_val_r > 0.5 else ("P" if type_val_r < -0.5 else "?")
                a_val = tensor_data[2, r, c]
                alpha_val = tensor_data[3, r, c]
                off_at_r = tensor_data[4, r, c]

                # 2. åå‘ c -> r
                type_val_c = tensor_data[1, c, r]
                type_str_c = "R" if type_val_c > 0.5 else ("P" if type_val_c < -0.5 else "?")
                off_at_c = tensor_data[4, c, r]

                print(
                    f"{r}->{c:<7} | {type_str_r:<5} | {a_val:<10.4f} | {alpha_val:<10.4f} | {off_at_r:<10.4f} (at {r})")
                print(f"{c}->{r:<7} | {type_str_c:<5} | {'^':<10} | {'^':<10} | {off_at_c:<10.4f} (at {c})")
                print("-" * 65)

    def _step3_select_tools(self, task_template, topology_data):
        print(f"\n--- Step 3: åŠ¨æ€é€‰æ‹©ä¼˜åŒ–ä¸è¯„ä¼°å·¥å…· (Structured Tool Library) ---")

        try:
            schema_str = json.dumps(ToolSelectionResponse.model_json_schema(), indent=2, ensure_ascii=False)
            tools_context = json.dumps(AVAILABLE_TOOLS_DEF, indent=2, ensure_ascii=False)
            full_system_prompt = TOOL_SELECTION_SYSTEM_PROMPT + STRUCTURED_OUTPUT_SUFFIX.format(schema=schema_str)

            user_content = f"""
                            ä»»åŠ¡ä¿¡æ¯:
                            {json.dumps(task_template, ensure_ascii=False)}
                            å½“å‰æ‹“æ‰‘:
                            {json.dumps(topology_data, ensure_ascii=False)}
                            === å¯ç”¨å·¥å…·åº“å®šä¹‰ (JSON) ===
                            {tools_context}
                            è¯·æ ¹æ®ä»»åŠ¡éœ€æ±‚é€‰æ‹©å·¥å…·ï¼Œå¹¶æŒ‰ç…§ Schema æ ¼å¼è¾“å‡ºã€‚
                            """

            self._record("Step 3 Tool Selection Input", user_content)

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": full_system_prompt},
                    {"role": "user", "content": user_content}
                ],
                response_format={"type": "json_object"}
            )

            content = response.choices[0].message.content
            response_obj = ToolSelectionResponse.model_validate_json(content)

            print(f"ğŸ¤” [Reasoning]: {response_obj.reasoning}")
            print(f"ğŸ”§ [Optimizer] Selected: {response_obj.selected_optimization_tools}")
            print(f"âš–ï¸ [Evaluator] Selected: {response_obj.selected_evaluation_tools}")

            if response_obj.suggested_new_optimization_tools:
                print(f"ğŸ’¡ [New Opt Idea] å»ºè®®æ–°å¢ä¼˜åŒ–å·¥å…·: {len(response_obj.suggested_new_optimization_tools)} ä¸ª")

            return {
                "selected_optimization_losses": response_obj.selected_optimization_tools,
                "selected_evaluation_metrics": response_obj.selected_evaluation_tools,
                "full_response": response_obj.model_dump()
            }

        except Exception as e:
            print(f"âŒ [Step 3 Error] å·¥å…·é€‰æ‹©å¤±è´¥: {e}")
            traceback.print_exc()
            return {
                "selected_optimization_losses": ["closure_loop", "regularization"],
                "selected_evaluation_metrics": ["dof_check"]
            }

    def _step4_optimize(self, tensor_data, task_template, tools_config):
        print(f"\n--- Step 4: æ‰§è¡Œä¼˜åŒ–å¾ªç¯ (Epochs={self.cfg.physics.max_iterations}) ---")

        selected_tool_names = tools_config.get('selected_optimization_losses', [])
        full_response = tools_config.get('full_response', {})
        new_tool_definitions = full_response.get('suggested_new_optimization_tools', [])

        if not new_tool_definitions:
            print(f"    -> [Optimizer] ä¼ å…¥æ–°å·¥å…·å»ºè®®: [] (Empty)")
        else:
            print(f"    -> [Optimizer] ä¼ å…¥æ–°å·¥å…·å»ºè®®: {len(new_tool_definitions)} ä¸ª")

        # === æ ¸å¿ƒä¿®æ”¹ï¼šæ¥æ”¶æ—¥å¿—è¿”å› ===
        optimized_geometry, optimized_q, opt_log_str = self.optimizer.run_optimization(
            tensor_data,
            task_template,
            selected_tool_names=selected_tool_names,
            new_tools_definitions=new_tool_definitions
        )

        # è®°å½•ä¼˜åŒ–æ—¥å¿—
        self._record("Step 4 Optimizer Log", opt_log_str)

        return optimized_geometry, opt_log_str

    def _step5_evaluate(self, optimized_tensor, tools_config):
        print(f"\n--- Step 5: ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š ---")
        return self.evaluator.generate_report(
            optimized_tensor,
            tools_config.get('selected_evaluation_metrics', [])
        )

    def _step6_store_experience(self, user_input, task_template, tensor_data, report):
        print(f"\n--- Step 6: å­˜å…¥ç»éªŒæ±  ---")
        self.memory.store_experience(user_input, task_template, tensor_data, report)

    def _step_reflect(self, task, topology_response, report):
        print(f"\n--- Step Reflect: å¤±è´¥åˆ†æä¸ç­–ç•¥è°ƒæ•´ ---")
        try:
            # 1. å‡†å¤‡ä¸Šä¸‹æ–‡ (å…¨é‡å†å²)
            full_history_text = "\n".join(self.trace_log)

            context = {
                "task_intent": task['user_intent_summary'],
                "current_status": {
                    "topology_nodes": len(topology_response.topology.nodes),
                    "ground": topology_response.meta.ground_nodes,
                    "ee": topology_response.meta.ee_node
                },
                "evaluation_report": report,
                "execution_history_trace": "See full logs provided below"
            }

            schema_str = json.dumps(ReflectionResponse.model_json_schema(), indent=2, ensure_ascii=False)
            system_prompt = REFLECTION_SYSTEM_PROMPT + STRUCTURED_OUTPUT_SUFFIX.format(schema=schema_str)

            user_prompt = f"""
            å½“å‰ä¼˜åŒ–ç»“æœä¸ç†æƒ³ã€‚ä»¥ä¸‹æ˜¯å®Œæ•´çš„æ‰§è¡Œå†å²è®°å½•ï¼ˆåŒ…å« Promptã€Raw Outputã€Code Logicã€Optimization Logsï¼‰ï¼š

            === EXECUTION HISTORY START ===
            {full_history_text}
            === EXECUTION HISTORY END ===

            è¯·ç»“åˆä¸Šè¿°å†å²ï¼Œåˆ†æå¤±è´¥åŸå› å¹¶ç»™å‡º JSON æ ¼å¼çš„ä¿®æ­£ç­–ç•¥ã€‚
            Context Summary: {json.dumps(context, ensure_ascii=False)}
            """

            # 2. è°ƒç”¨ LLM
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                response_format={"type": "json_object"}
            )

            # 3. è§£æ
            content = response.choices[0].message.content
            self._record("Step Reflect Output", content)

            reflection = ReflectionResponse.model_validate_json(content)

            print(f"ğŸ§  [Analysis]: {reflection.analysis}")
            print(f"ğŸ‘‰ [Action]: {reflection.action.value}")

            return reflection

        except Exception as e:
            print(f"âŒ [Reflection Error] åæ€å¤±è´¥ï¼Œé»˜è®¤é‡ç½®åˆå€¼: {e}")
            traceback.print_exc()
            return ReflectionResponse(
                analysis="Reflection failed",
                action=ReflectionAction.REINIT_GEOMETRY
            )
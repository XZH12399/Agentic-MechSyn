import torch
from torch.utils.data import Dataset
from dataset_builder.tokenizer import MechanismTokenizer

class LLMMechanismDataset(Dataset):
    def __init__(self, pt_file_path, llm_tokenizer, max_length=1024):
        """
        :param pt_file_path: ä½ çš„ train_dataset.pt è·¯å¾„
        :param llm_tokenizer: Qwen2-VL çš„åˆ†è¯å™¨
        """
        # 1. åŠ è½½ä½ ç”Ÿæˆçš„åŸå§‹ .pt æ•°æ®
        print(f"ğŸ“‚ Loading PT data from {pt_file_path}...")
        checkpoint = torch.load(pt_file_path)
        self.src_data = checkpoint['src'] # è¾“å…¥: Specs + Screws
        self.tgt_data = checkpoint['tgt'] # è¾“å‡º: æœºæ„ Graph
        
        # 2. æ¢å¤ä½ çš„ç‰©ç† Tokenizer (ä¸ºäº†æŠŠ ID å˜å›å­—ç¬¦ä¸²)
        self.mech_tokenizer = MechanismTokenizer()
        # å¼ºåˆ¶åŒæ­¥è¯è¡¨ (éå¸¸é‡è¦!)
        self.mech_tokenizer.vocab = checkpoint['vocab']
        self.mech_tokenizer.id2token = {i: t for i, t in enumerate(self.mech_tokenizer.vocab)}
        
        self.llm_tokenizer = llm_tokenizer
        self.max_length = max_length
        
        # é¢„è®¡ç®— Prompt æ¨¡æ¿
        self.system_prompt = "You are an expert in mechanism design. Generate a mechanism topology based on the given motion requirements."

    def __len__(self):
        return len(self.src_data)

    def __getitem__(self, idx):
        # === A. è¿˜åŸæ•°æ® ===
        # æŠŠ ID [10, 25...] å˜æˆå­—ç¬¦ä¸² ["<DoF_1>", "<Off_0.5>..."]
        
        # 1. å¤„ç†è¾“å…¥ (Source)
        raw_src = self.src_data[idx].tolist()
        src_tokens = [self.mech_tokenizer.id2token.get(i, "") for i in raw_src if i != 0] # å»æ‰ PAD
        src_str = "".join(src_tokens) # è¿æˆä¸€æ•´æ¡å­—ç¬¦ä¸²
        
        # 2. å¤„ç†è¾“å‡º (Target)
        raw_tgt = self.tgt_data[idx].tolist()
        tgt_tokens = [self.mech_tokenizer.id2token.get(i, "") for i in raw_tgt if i != 0]
        tgt_str = "".join(tgt_tokens)
        
        # === B. æ„å»ºå¯¹è¯æ ¼å¼ ===
        # Qwen2-VL æ¨èçš„å¯¹è¯æ ¼å¼
        conversation = [
            {
                "role": "system",
                "content": self.system_prompt
            },
            {
                "role": "user",
                "content": f"Design Specs: {src_str}"
            },
            {
                "role": "assistant",
                "content": tgt_str
            }
        ]
        
        # === C. ä½¿ç”¨ LLM çš„æ¨¡æ¿å·¥å…·è¿›è¡Œç¼–ç  ===
        # apply_chat_template ä¼šè‡ªåŠ¨å¤„ç† <|im_start|>user... ç­‰ç‰¹æ®Šç¬¦
        text = self.llm_tokenizer.apply_chat_template(
            conversation,
            tokenize=False,
            add_generation_prompt=False
        )
        
        # ç¼–ç ä¸º Tensor
        encoding = self.llm_tokenizer(
            text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        
        input_ids = encoding.input_ids.squeeze(0)
        attention_mask = encoding.attention_mask.squeeze(0)
        
        # === D. æ„é€  Labels (Loss è®¡ç®—æ©ç ) ===
        # æˆ‘ä»¬åªè®­ç»ƒ Assistant å›å¤çš„éƒ¨åˆ†ï¼ŒMask æ‰ System å’Œ User çš„éƒ¨åˆ†
        labels = input_ids.clone()
        
        # ç®€å•ç­–ç•¥ï¼šæ‰¾åˆ° "assistant" æ ‡ç­¾åçš„å†…å®¹å¼€å§‹è®­ç»ƒ
        # Qwen2-VL çš„ assistant å¼•å¯¼ç¬¦é€šå¸¸åŒ…å« "\n<|im_start|>assistant\n"
        # è¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬è®©æ¨¡å‹å…¨é‡é¢„æµ‹ï¼ˆPrompt Loss ä¸ä¼šå¤ªå¤§å½±å“ï¼‰ï¼Œæˆ–è€…ä½¿ç”¨ DataCollatorMask
        # å¯¹äºåˆå­¦è€…ï¼Œç›´æ¥è®© labels = input_ids ä¹Ÿæ˜¯å¯ä»¥è·‘é€šçš„ï¼Œåªæ˜¯æ•ˆç‡ç•¥ä½
        # ä¸ºäº†ä¸¥è°¨ï¼Œæˆ‘ä»¬å°† Padding éƒ¨åˆ†è®¾ä¸º -100
        labels[labels == self.llm_tokenizer.pad_token_id] = -100
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }
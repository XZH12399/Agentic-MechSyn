import os

# ================= 1. æ ¸å¼¹çº§ç¦»çº¿é…ç½® (å¿…é¡»æ”¾åœ¨æœ€å‰é¢) =================
# å¼ºåˆ¶è®© huggingface_hub åº“è®¤ä¸ºè‡ªå·±åœ¨æ–­ç½‘ç¯å¢ƒï¼Œç¦æ­¢ä¸€åˆ‡ metadata æŸ¥è¯¢
os.environ["HF_HUB_OFFLINE"] = "1" 
os.environ["HF_DATASETS_OFFLINE"] = "1"

# æŒ‡å®šæ˜¾å¡
os.environ["CUDA_VISIBLE_DEVICES"] = "1" 

import torch
from transformers import AutoTokenizer, Qwen2VLForConditionalGeneration

# ================= 2. æ¨¡å‹è·¯å¾„ =================
BASE_MODEL_ID = "/home/XZH/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c"

def load_base_model():
    print(f"ğŸš€ Loading Base Model from {BASE_MODEL_ID}...")
    print("   (Mode: Strictly Offline / Force Cache)")
    
    # 1. åŠ è½½åŸå§‹ Tokenizer
    # æ³¨æ„ï¼šæœ‰äº†ä¸Šé¢çš„ HF_HUB_OFFLINE=1ï¼Œè¿™é‡Œçš„ local_files_only å…¶å®æ˜¯åŒä¿é™©
    tokenizer = AutoTokenizer.from_pretrained(
        BASE_MODEL_ID, 
        trust_remote_code=True,
        local_files_only=True 
    )

    # 2. åŠ è½½åŸå§‹æ¨¡å‹
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        BASE_MODEL_ID,
        dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True 
    )
    
    model.eval()
    print("âœ… Base Model Loaded Successfully!")
    return model, tokenizer

def generate_text(model, tokenizer, prompt_text):
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": prompt_text}
    ]
    
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(text, return_tensors="pt").to(model.device)

    print(f"\nğŸ¤– User Asking: {prompt_text}")
    print("â³ Generating...")
    
    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.7,
            top_p=0.9
        )

    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    return output_text

# ================= 3. ä¸»æµ‹è¯•ç¨‹åº =================
if __name__ == "__main__":
    try:
        model, tokenizer = load_base_model()

        # æµ‹è¯• 1: é€šç”¨çŸ¥è¯† (è€ƒå¯Ÿå®ƒåŸæœ¬çŸ¥ä¸çŸ¥é“ Bennett)
        prompt_1 = "ä½ çŸ¥é“Bennettç©ºé—´æœºæ„æ˜¯ä»€ä¹ˆå—ï¼Ÿ"
        response_1 = generate_text(model, tokenizer, prompt_1)
        print(f"\nğŸ’¬ Base Model Response 1:\n{'-'*50}\n{response_1}\n{'-'*50}")

    except Exception as e:
        print("\nâŒ ä¾ç„¶æŠ¥é”™ï¼Ÿå°è¯•å¤‡ç”¨æ–¹æ¡ˆï¼š")
        print(f"Error: {e}")
        print("\nğŸ’¡ æç¤ºï¼šå¦‚æœä¾ç„¶æŠ¥é”™ï¼Œè¯·ä½¿ç”¨ 'huggingface-cli scan-cache' æ‰¾åˆ°æ¨¡å‹çš„çœŸå®ç»å¯¹è·¯å¾„ï¼Œæ›¿æ¢ BASE_MODEL_IDã€‚")
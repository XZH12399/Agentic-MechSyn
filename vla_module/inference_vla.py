import os
import re
import torch
import glob
from transformers import AutoTokenizer, Qwen2VLForConditionalGeneration
from peft import PeftModel

# ================= ğŸ›ï¸ 1. æ¨¡å‹åˆ‡æ¢ä¸ç¡¬ä»¶é…ç½® =================
USE_MODEL = "7B" 

# é”å®šæ˜¾å¡ 1
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
# å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
os.environ["HF_HUB_OFFLINE"] = "1"

# --- è‡ªåŠ¨å¯»æ‰¾ 7B æ¨¡å‹ç¼“å­˜è·¯å¾„ ---
def get_7b_cache_path():
    # æ ‡å‡† Hugging Face ç¼“å­˜æ ¹ç›®å½•
    cache_base = "/home/XZH/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/*"
    paths = glob.glob(cache_base)
    if not paths:
        return "/home/XZH/projects/Agentic-MechSyn/Qwen2-VL-7B-Instruct" # å›é€€åˆ°é¡¹ç›®ç›®å½•
    return paths[0] # è¿”å›æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªå“ˆå¸Œå¿«ç…§è·¯å¾„

if USE_MODEL == "7B":
    print("ğŸš€ Inference Mode: [7B] High Intelligence")
    # è‡ªåŠ¨è·å–ç±»ä¼¼ /home/XZH/.cache/huggingface/hub/.../snapshots/xxxx çš„è·¯å¾„
    BASE_MODEL_ID = get_7b_cache_path()
    ADAPTER_PATH = "/mnt/sda/xzh/vla_checkpoints_7b/checkpoint-2814" 
else:
    print("ğŸš€ Inference Mode: [2B] Standard Speed")
    # è¿™é‡Œè¯·å¡«å…¥ä½ ä¹‹å‰ 2B çš„é‚£ä¸ªå®Œæ•´é•¿è·¯å¾„
    BASE_MODEL_ID = "/home/XZH/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/895c3a49bc3fa70a340399125c650a463535e71c"
    ADAPTER_PATH = "./vla_checkpoints/checkpoint-xxx"

# ================= 2. åŠ è½½å‡½æ•° =================
def load_model_and_tokenizer():
    # é¢„æ£€è·¯å¾„
    abs_base_path = os.path.abspath(BASE_MODEL_ID)
    abs_adapter_path = os.path.abspath(ADAPTER_PATH)

    if not os.path.exists(abs_base_path):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°åº•åº§æ¨¡å‹ç»å¯¹è·¯å¾„: {abs_base_path}")
    if not os.path.exists(abs_adapter_path):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ° Adapter è·¯å¾„: {abs_adapter_path}")

    print(f"ğŸ“¦ Loading Tokenizer from: {abs_base_path}")
    tokenizer = AutoTokenizer.from_pretrained(
        abs_base_path, 
        trust_remote_code=True,
        local_files_only=True
    )

    print(f"ğŸ“¦ Loading Base Model from: {abs_base_path}")
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        abs_base_path,
        torch_dtype=torch.bfloat16,
        device_map="auto", 
        trust_remote_code=True,
        local_files_only=True
    )

    if len(tokenizer) > model.get_input_embeddings().weight.shape[0]:
        print("ğŸ”§ Resizing token embeddings...")
        model.resize_token_embeddings(len(tokenizer))

    print(f"ğŸ”— Loading LoRA Adapter: {abs_adapter_path}")
    model = PeftModel.from_pretrained(model, abs_adapter_path)
    
    model.eval()
    print("âœ… Model loaded successfully from absolute path!")
    return model, tokenizer

# ================= 3. æ¨ç†ç”Ÿæˆå‡½æ•° =================
def generate_mechanism(model, tokenizer, prompt_text):
    messages = [
        {"role": "system", "content": "You are a helpful assistant for mechanism design."},
        {"role": "user", "content": prompt_text}
    ]
    
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(text, return_tensors="pt").to(model.device)

    print(f"\nğŸ’¬ Input Prompt: {prompt_text}")
    print("â³ Generating...")
    
    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=1024,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1
        )

    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)]
    output_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return output_text

# ================= 4. è§£ç å¯è§†åŒ–æŠ¥å‘Š =================
def parse_and_print_mechanism(token_string):
    print("\n" + "="*115)
    print("ğŸ§© æœºæ„å…¨æ¯è§£ç æŠ¥å‘Š (Agentic-MechSyn Report)")
    print("="*115)
    
    header = f"{'æ‹“æ‰‘åŠ¨ä½œ (Topology)':<35} | {'å…¬å…±å‚çº¿ (Common Normal)':<24} | {'æºè½´ (Source Axis)':<24} | {'ç›®æ ‡è½´ (Target Axis)':<24}"
    print(header)
    print("-" * 115)

    all_actions = list(re.finditer(r"(<Action_[a-zA-Z_]+>)", token_string))
    param_pattern = re.compile(r"<([a-zA-Z]+)_([a-zA-Z0-9]+)>")

    current_source = "Base"

    for i, match in enumerate(all_actions):
        action_tag = match.group(1)
        start_idx = match.end()
        end_idx = all_actions[i+1].start() if i+1 < len(all_actions) else len(token_string)
        content_str = token_string[start_idx:end_idx]
        
        params = param_pattern.findall(content_str)
        p_dict = {'ID': '?', 'Type': '', 'Role': ''}
        geo_vals = {'Len': [], 'Twist': [], 'Off': [], 'State': []}
        
        for key, val in params:
            if key in p_dict: p_dict[key] = val
            elif key in geo_vals: geo_vals[key].append(val)

        a_val = geo_vals['Len'][0] if geo_vals['Len'] else "-"
        alpha_val = geo_vals['Twist'][0] if geo_vals['Twist'] else "-"
        d_src = geo_vals['Off'][0] if len(geo_vals['Off']) > 0 else "-"
        d_tgt = geo_vals['Off'][1] if len(geo_vals['Off']) > 1 else "-"
        theta_src = geo_vals['State'][0] if len(geo_vals['State']) > 0 else "-"
        theta_tgt = geo_vals['State'][1] if len(geo_vals['State']) > 1 else "-"

        common_str = f"a={a_val}, Î±={alpha_val}"
        src_str = f"ds={d_src}, Î¸s={theta_src}"
        tgt_str = f"dt={d_tgt}, Î¸t={theta_tgt}"

        target_id = p_dict['ID']
        type_info = f"[{p_dict['Type']}{'-' + p_dict['Role'] if p_dict['Role'] else ''}]"
        
        action_desc = ""
        if action_tag == "<Action_New_Node>":
            action_desc = f"ğŸ”¹ åŸºåº§ {target_id} {type_info}"
            current_source = target_id
            common_str = src_str = tgt_str = ""
        elif action_tag == "<Action_Link_To_New>":
            action_desc = f" â”œâ”€â”€ ğŸ”— {current_source} -> {target_id} {type_info}"
            current_source = target_id
        elif action_tag == "<Action_Link_To_Old>":
            action_desc = f" â””â”€â”€ ğŸ”„ é—­ç¯ {current_source} -> {target_id}"
        elif action_tag == "<Action_Jump_To>":
            action_desc = f"ğŸš€ è·³è½¬ç„¦ç‚¹ -> {target_id}"
            current_source = target_id
            common_str = src_str = tgt_str = ""

        if action_desc:
            print(f"{action_desc:<35} | {common_str:<24} | {src_str:<24} | {tgt_str:<24}")
    print("="*115)

# ================= 5. ä¸»ç¨‹åº =================
if __name__ == "__main__":
    try:
        model, tokenizer = load_model_and_tokenizer()
        # test_prompt = "æˆ‘è€å©†è®©æˆ‘Design a mechanism with 1 loopï¼Œåº”è¯¥æ€ä¹ˆå›å¤ã€‚"
        # test_prompt = "ä»Šå¤©æ˜¯è·¨å¹´å¤œï¼Œæˆ‘é€‚åˆç»™è€å©†ä¹°ä»€ä¹ˆç¤¼ç‰©ï¼Ÿ"
        test_prompt = "Design a mechanism with 2 DoFs and 3 loops."
        # test_prompt = "Design a bennett mechanism."
        # test_prompt = "ä½ çŸ¥é“Bennettæœºæ„æ˜¯ä»€ä¹ˆå—ï¼Ÿä»¥åŠè¿™ä¸ªæœºæ„æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ"
        # test_prompt = "å…ˆæœ‰é¸¡è¿˜æ˜¯å…ˆæœ‰è›‹ã€‚"
        # test_prompt = "è¯·ç»™æˆ‘è®¾è®¡ä¸€ä¸ªåƒé¸¡è›‹ä¸€æ ·çš„æœºæ„ã€‚"
        result = generate_mechanism(model, tokenizer, test_prompt)
        print(f"\nğŸ› ï¸  Raw Token Stream:\n{result}")
        parse_and_print_mechanism(result)
    except Exception as e:
        print(f"âŒ è¿è¡Œå¤±è´¥: {e}")
import os
import glob
import sys
from dotenv import load_dotenv

# 1. ÂÖàÂä†ËΩΩ .env Êñá‰ª∂‰∏≠ÁöÑÁéØÂ¢ÉÂèòÈáè
# load_dotenv() ‰ºöËá™Âä®ÂØªÊâæÊ†πÁõÆÂΩï‰∏ãÁöÑ .env Êñá‰ª∂ÔºåÂπ∂Â∞ÜÂÜÖÂÆπÊ≥®ÂÖ•Âà∞ os.environ ‰∏≠
load_dotenv() 

# ================= 1. Ë∫´‰ªΩ‰∏éÁΩëÁªúÈÖçÁΩÆ (Âú®ÂØºÂÖ• torch ‰πãÂâç) =================
# Ê≠§Êó∂ os.environ ‰∏≠Â∑≤ÁªèÊúâ‰∫Ü HF_TOKEN Á≠âÂèòÈáèÔºåÊÇ®ÂèØ‰ª•Áõ¥Êé•ÁªßÁª≠ÂêéÈù¢ÁöÑÂØºÂÖ•
# Â¶ÇÊûúÊÇ®ÊÉ≥Á°Æ‰øùÊüê‰∫õÂèòÈáèÂøÖÈ°ªÂ≠òÂú®ÔºåÂèØ‰ª•Âä†‰∏Ä‰∏™ÁÆÄÂçïÁöÑÊ£ÄÊü•ÔºàÂèØÈÄâÔºâÔºö
if not os.getenv("HF_TOKEN"):
    print("Ë≠¶Âëä: Êú™Ê£ÄÊµãÂà∞ HF_TOKENÔºåËØ∑Ê£ÄÊü• .env Êñá‰ª∂")

import torch
import json
from datasets import load_dataset
from transformers import (
    AutoTokenizer, 
    Qwen2VLForConditionalGeneration, 
    TrainingArguments, 
    Trainer,
    DataCollatorForSeq2Seq,
    TrainerCallback
)
from peft import LoraConfig, get_peft_model, TaskType

# ================= üéõÔ∏è Ê†∏ÂøÉÈÖçÁΩÆÂºÄÂÖ≥ =================
USE_MODEL = "7B" 
DATA_PATH = "dataset_builder/output/balanced_dataset/train_dataset_natural.json"

# --- üõ∞Ô∏è Ëá™Âä®ÂØªÊâæÊ®°ÂûãÁºìÂ≠òË∑ØÂæÑÈÄªËæë ---
def get_model_path(model_type="7B"):
    if model_type == "7B":
        cache_base = "/home/XZH/.cache/huggingface/hub/models--Qwen--Qwen2-VL-7B-Instruct/snapshots/*"
        fallback = "/home/XZH/projects/Agentic-MechSyn/Qwen2-VL-7B-Instruct"
    else:
        cache_base = "/home/XZH/.cache/huggingface/hub/models--Qwen--Qwen2-VL-2B-Instruct/snapshots/*"
        fallback = "/home/XZH/projects/Agentic-MechSyn/Qwen2-VL-2B-Instruct"
        
    paths = glob.glob(cache_base)
    if paths:
        # ÊâæÂà∞ÂìàÂ∏åË∑ØÂæÑÔºåËøîÂõûÁ¨¨‰∏Ä‰∏™
        resolved_path = paths[0]
        print(f"üîç Found cached model at: {resolved_path}")
        return resolved_path
    else:
        print(f"‚ö†Ô∏è No cache found, using fallback: {fallback}")
        return fallback

# ================= üõ†Ô∏è Ê¢ØÂ∫¶ÁõëÊéßÂõûË∞É =================
class CheckGradCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None and "grad_norm" in logs:
            grad = logs["grad_norm"]
            if grad > 10.0:
                print(f"\n‚ö†Ô∏è  [WARN] Step {state.global_step}: Grad_norm high ({grad:.2f})")
            if grad > 1000.0:
                print(f"\nüö® [DANGER] Step {state.global_step}: Gradient Explosion! ({grad:.2e})")

# === 2. ÂèÇÊï∞Ëá™Âä®ÈÖçÁΩÆ ===
MODEL_ID = get_model_path(USE_MODEL)

if USE_MODEL == "7B":
    print(f"üöÄ [Mode] Qwen2-VL-7B High-Stability Tuning")
    OUTPUT_DIR = "/mnt/sda/xzh/vla_checkpoints_7b"
    PER_DEVICE_BATCH_SIZE = 2      
    GRADIENT_ACCUMULATION = 16      
    LEARNING_RATE = 2e-5            
    WARMUP_RATIO = 0.15             
    MAX_GRAD_NORM = 0.5             
    LORA_R = 64
    LORA_ALPHA = 128
else:
    print(f"üöÄ [Mode] Qwen2-VL-2B Standard Tuning")
    OUTPUT_DIR = "/mnt/sda/xzh/vla_checkpoints"
    PER_DEVICE_BATCH_SIZE = 8
    GRADIENT_ACCUMULATION = 4
    LEARNING_RATE = 1e-4
    WARMUP_RATIO = 0.1
    MAX_GRAD_NORM = 1.0
    LORA_R = 16
    LORA_ALPHA = 32

# ================= 3. Êï∞ÊçÆÂ§ÑÁêÜ =================
def process_func(example, tokenizer):
    MAX_LENGTH = 1024 
    instruction = example["instruction"]
    output = example["output"]
    messages = [
        {"role": "system", "content": "You are a helpful assistant for mechanism design."},
        {"role": "user", "content": instruction},
        {"role": "assistant", "content": output}
    ]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
    model_inputs = tokenizer(text, max_length=MAX_LENGTH, padding="max_length", truncation=True, return_tensors="pt")
    input_ids = model_inputs.input_ids[0]
    attention_mask = model_inputs.attention_mask[0]
    
    user_messages = messages[:-1]
    user_text = tokenizer.apply_chat_template(user_messages, tokenize=False, add_generation_prompt=True)
    user_input_ids = tokenizer(user_text, add_special_tokens=False).input_ids
    
    len_user_prompt = len(user_input_ids)
    labels = input_ids.clone()
    labels[:len_user_prompt] = -100
    labels[input_ids == tokenizer.pad_token_id] = -100
    return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}

def load_and_process_data(tokenizer):
    print(f"üìÇ Loading: {DATA_PATH}")
    ds = load_dataset("json", data_files=DATA_PATH, split="train")
    return ds.map(lambda x: process_func(x, tokenizer), remove_columns=ds.column_names, num_proc=4)

# ================= 4. Ê®°ÂûãÂä†ËΩΩ =================
def load_model():
    print(f"üì¶ Loading Tokenizer/Model from: {MODEL_ID}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True, local_files_only=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = Qwen2VLForConditionalGeneration.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.bfloat16,
        attn_implementation="eager", 
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True
    )
    model.gradient_checkpointing_enable()
    return model, tokenizer

def apply_lora(model):
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=0.05,
        target_modules=target_modules, bias="none",
    )
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    return model

# ================= 5. ËÆ≠ÁªÉÊâßË°å =================
def train():
    model, tokenizer = load_model()
    model = apply_lora(model)
    train_dataset = load_and_process_data(tokenizer)
    
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION,
        learning_rate=LEARNING_RATE,
        lr_scheduler_type="cosine",
        warmup_ratio=WARMUP_RATIO,
        max_grad_norm=MAX_GRAD_NORM,
        num_train_epochs=3,
        logging_strategy="steps",
        logging_steps=5,
        save_strategy="steps",
        save_steps=100,
        save_total_limit=3,
        bf16=True,
        gradient_checkpointing=True,
        report_to="tensorboard",
        remove_unused_columns=False,
        log_level="info"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
        callbacks=[CheckGradCallback()]
    )
    
    print("üöÄ Training Started...")
    trainer.train()
    trainer.save_model(output_dir=OUTPUT_DIR)
    print(f"‚úÖ Saved to {OUTPUT_DIR}")

if __name__ == "__main__":
    train()
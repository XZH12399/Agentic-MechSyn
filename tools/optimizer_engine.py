import torch
import time
import json
import numpy as np
from tools.physics_kernel import PhysicsKernel
from tools.tool_registry import AVAILABLE_TOOLS_DEF

class MechanismOptimizer:
    def __init__(self, physics_config):
        self.cfg = physics_config
        self.lr = physics_config.learning_rate
        self.epochs = physics_config.max_iterations
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.kernel = PhysicsKernel(self.device, self.cfg)

        # =====================================================
        # ğŸ”§ åŠ¨æ€åŠ è½½ä¼˜åŒ–å·¥å…· (è‡ªåŠ¨åŒ–ç»‘å®š)
        # =====================================================
        self.loss_tools = {}
        
        # éå†æ³¨å†Œè¡¨ä¸­çš„ optimizer åˆ—è¡¨
        for tool_def in AVAILABLE_TOOLS_DEF.get("optimizer", []):
            tool_name = tool_def["name"]
            method_name = tool_def.get("binding_method")
            
            if method_name and hasattr(self.kernel, method_name):
                self.loss_tools[tool_name] = getattr(self.kernel, method_name)
            else:
                print(f"âš ï¸ [Optimizer] Warning: Method '{method_name}' for tool '{tool_name}' not found in PhysicsKernel.")

    def run_optimization(self, tensor_data, task_dict, selected_tools, new_tools=None, initial_q=None):
        """
        æ‰§è¡Œå‡ ä½•å‚æ•°ä¼˜åŒ–
        """
        print(f"    -> [Optimizer] å¯åŠ¨ PyTorch ååŒä¼˜åŒ–å¼•æ“ (Device: {self.device})")
        
        # 1. æ•°æ®å‡†å¤‡
        # å°† numpy è½¬ä¸ºå¯ä¼˜åŒ–çš„ Tensor
        if isinstance(tensor_data, np.ndarray):
            tensor = torch.tensor(tensor_data, dtype=torch.float32, device=self.device)
        else:
            tensor = tensor_data.to(self.device)
            
        tensor.requires_grad = True

        # 2. å‡†å¤‡ Q (å…³èŠ‚çŠ¶æ€)
        if initial_q is not None:
            if isinstance(initial_q, np.ndarray):
                q_opt = torch.tensor(initial_q, dtype=torch.float32, device=self.device)
            else:
                q_opt = initial_q.to(self.device)
            print("        âœ… [Init] ä½¿ç”¨ä¸Šæ¸¸æä¾›çš„åˆå§‹ä½å‹ (Initial Guess)ã€‚")
        else:
            q_opt = torch.zeros((tensor.shape[1], tensor.shape[2]), dtype=torch.float32, device=self.device)
        
        # æ£€æµ‹å¹¶æ‰“ç ´å…¨0å¯¹ç§°æ€§
        if torch.all(q_opt == 0):
            print("        âš ï¸ [Init] æ£€æµ‹åˆ°åˆå§‹ Q å…¨ä¸º 0ï¼Œæ·»åŠ å¾®å°æ‰°åŠ¨ä»¥æ‰“ç ´å¯¹ç§°æ€§ã€‚")
            q_opt += torch.randn_like(q_opt) * 0.01

        q_opt.requires_grad = True

        # 3. æ‹“æ‰‘åˆ†æ (ç”¨äº Loss è®¡ç®—)
        adj = tensor[0].detach()
        cycles = self.kernel.find_fundamental_cycles(adj)
        print(f"    -> [Optimizer] æ‹“æ‰‘åˆ†æ: å‘ç° {len(cycles)} ä¸ªåŸºæœ¬é—­ç¯")

        # 4. ä¼˜åŒ–å™¨é…ç½®
        optimizer = torch.optim.Adam([tensor, q_opt], lr=self.lr)
        
        # æ‰“å°ç›®æ ‡è¿åŠ¨èºæ—‹ (Debugç”¨)
        targets = task_dict.get('targets', {})
        target_twists = targets.get('target_motion_twists', [])
        if target_twists:
            desc = targets.get('description', 'Unknown')
            print(f"    -> [Optimizer] ğŸ¯ ç›®æ ‡è¿åŠ¨æ¨¡å¼ (Target Motion):")
            print(f"        - Description: {desc}")
            for i, tw in enumerate(target_twists):
                # æ ¼å¼åŒ–æ‰“å°ï¼Œæ–¹ä¾¿æ£€æŸ¥æ˜¯å¦è¿˜æ˜¯ [0,0,1...]
                fmt_tw = ", ".join([f"{x:.4f}" for x in tw])
                print(f"        - Mode {i+1} Expectation: [{fmt_tw}]")

        print(f"Optimizer Config: Device={self.device}, LR={self.lr}, Epochs={self.epochs}")

        # 5. ä¼˜åŒ–å¾ªç¯
        history = []
        
        for epoch in range(1, self.epochs + 1):
            optimizer.zero_grad()
            
            total_loss = torch.tensor(0.0, device=self.device)
            loss_components = {}

            # è®¡ç®—å„é¡¹ Loss
            for tool_name in selected_tools:
                if tool_name in self.loss_tools:
                    loss_func = self.loss_tools[tool_name]
                    try:
                        val = loss_func(tensor, task_dict, cycles, q_opt)
                        total_loss += val
                        loss_components[tool_name] = val.item()
                    except Exception as e:
                        print(f"Error in {tool_name}: {e}")

            # åå‘ä¼ æ’­
            total_loss.backward()
            
            # âœ¨âœ¨âœ¨ æ ¸å¿ƒä¿®æ­£: å†»ç»“æ‹“æ‰‘æ¢¯åº¦ âœ¨âœ¨âœ¨
            self._freeze_topology_gradients(tensor)

            optimizer.step()

            # è®°å½•ä¸æ‰“å°
            if epoch == 1 or epoch % 50 == 0:
                log_str = f"        Epoch {epoch}: Loss={total_loss.item():.4f} (LR={self.lr:.1e})"
                for k, v in loss_components.items():
                    log_str += f" | {k}={v:.4f}"
                print(log_str)
                history.append(log_str)

        return tensor.detach(), q_opt.detach(), "\n".join(history)

    def _freeze_topology_gradients(self, tensor):
        """
        ğŸ”’ å†»ç»“æ‹“æ‰‘ç›¸å…³çš„æ¢¯åº¦ï¼Œé˜²æ­¢ä¼˜åŒ–å™¨ä¿®æ”¹æœºæ„ç»“æ„ã€‚
        
        Tensor Channels:
        [0]: Adjacency (è¿æ¥å…³ç³») -> å¿…é¡»å†»ç»“
        [1]: Joint Type (å…³èŠ‚ç±»å‹) -> å¿…é¡»å†»ç»“ (é˜²æ­¢ P å˜ R)
        [2]: Link Length (a)      -> å¯ä¼˜åŒ–
        [3]: Twist Angle (alpha)  -> å¯ä¼˜åŒ–
        [4]: Offset (d/theta)     -> å¯ä¼˜åŒ–
        """
        if tensor.grad is not None:
            # å°† Channel 0 (Adj) å’Œ Channel 1 (Type) çš„æ¢¯åº¦å¼ºåˆ¶è®¾ä¸º 0
            tensor.grad[0] = 0.0
            tensor.grad[1] = 0.0
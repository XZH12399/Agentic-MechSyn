import torch
import numpy as np
import math
import networkx as nx
from collections import deque


class MechanismOptimizer:
    def __init__(self, physics_config):
        self.cfg = physics_config
        self.lr = physics_config.learning_rate
        self.epochs = physics_config.max_iterations
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # å·¥å…·æ³¨å†Œè¡¨
        self.loss_tools = {
            "closure_loop": self._loss_closure,
            "mobility_dof": self._loss_mobility_dof,
            "path_error": self._loss_path_generation,
            "twist_match": self._loss_twist_alignment,
            "bennett_ratio": self._loss_bennett_condition,
            "instantaneous_check": self._loss_instantaneous_check
        }

    def run_optimization(self, initial_tensor, task_template, selected_tool_names, new_tools_definitions=[]):
        print(f"    -> [Optimizer] å¯åŠ¨ PyTorch ååŒä¼˜åŒ–å¼•æ“ (Device: {self.device})")

        # æ–°å¢: æ—¥å¿—æ•è·åˆ—è¡¨
        execution_log = []

        def log_and_print(msg):
            print(msg)
            execution_log.append(msg)

        log_and_print(f"Optimizer Config: Device={self.device}, LR={self.lr}, Epochs={self.epochs}")

        N = initial_tensor.shape[1]

        # 1. Geometry (5, N, N)
        geometry_tensor = torch.tensor(
            initial_tensor, dtype=torch.float32, device=self.device, requires_grad=True
        )

        # 2. Joint Variables (N, N)
        q_opt = torch.empty((N, N), device=self.device).uniform_(-0.5, 0.5).requires_grad_(True)

        # 3. Optimizer & Scheduler
        optimizer = torch.optim.Adam([
            {'params': geometry_tensor, 'lr': self.lr},
            {'params': q_opt, 'lr': self.lr * 2.0}
        ], lr=self.lr)

        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=50, min_lr=1e-5
        )

        # Topology Analysis
        adj_matrix = initial_tensor[0]
        self.cycles = self._find_fundamental_cycles(adj_matrix)
        self.G_graph = self._build_nx_graph(adj_matrix)

        print(f"    -> [Optimizer] æ‹“æ‰‘åˆ†æ: å‘ç° {len(self.cycles)} ä¸ªåŸºæœ¬é—­ç¯")

        for epoch in range(1, self.epochs + 1):
            optimizer.zero_grad()
            total_loss = torch.tensor(0.0, device=self.device)
            loss_components = {}

            # constrained_geometry æ˜¯ç‰©ç†åˆæ³•çš„å¼ é‡ï¼Œä¸”ä¿ç•™äº†å¯¹ geometry_tensor çš„æ¢¯åº¦
            constrained_geometry = self._get_constrained_geometry(geometry_tensor)

            for tool_name in selected_tool_names:
                if tool_name in self.loss_tools:
                    loss_func = self.loss_tools[tool_name]

                    # æ³¨æ„ï¼šä¼ å…¥çš„æ˜¯ constrained_geometry
                    loss_val = loss_func(constrained_geometry, task_template, cycles=self.cycles, q_opt=q_opt)

                    weight = 1.0
                    if tool_name == "closure_loop": weight = 10.0
                    if tool_name == "bennett_ratio": weight = 10.0
                    if tool_name in ["path_error", "twist_match"]: weight = 50.0
                    if tool_name == "mobility_dof": weight = 10.0

                    total_loss += weight * loss_val
                    loss_components[tool_name] = loss_val.item()

            # å¦‚æœ total_loss æ²¡æœ‰æ¢¯åº¦å‡½æ•° (å³å®ƒæ˜¯ä¸€ä¸ªå¸¸æ•°)ï¼Œè¯´æ˜å½“å‰æ‰€æœ‰çš„ Loss éƒ½å¤±æ•ˆäº†
            if total_loss.grad_fn is None:
                # æ‰‹åŠ¨åŠ ä¸Šä¸€ä¸ªä¸å‚æ•°ç›¸å…³çš„ 0.0ï¼Œå¼ºè¡Œå»ºç«‹è®¡ç®—å›¾è¿æ¥
                # è¿™æ · backward() å°±ä¸ä¼šæŠ¥é”™ï¼Œåªæ˜¯æ¢¯åº¦ä¸º 0
                dummy = (geometry_tensor.sum() + q_opt.sum()) * 0.0
                total_loss = total_loss + dummy

            total_loss.backward()

            torch.nn.utils.clip_grad_norm_([geometry_tensor, q_opt], max_norm=1.0)

            optimizer.step()
            scheduler.step(total_loss.item())

            # é˜²æ­¢ float è¯¯å·®å¯¼è‡´è¿æ¥æ–­å¼€æˆ–ç±»å‹çªå˜
            with torch.no_grad():
                geometry_tensor[0] = torch.tensor(initial_tensor[0], device=self.device)
                geometry_tensor[1] = torch.tensor(initial_tensor[1], device=self.device)

            if epoch % 50 == 0 or epoch == 1:
                current_lr = optimizer.param_groups[0]['lr']
                log_str = f"Epoch {epoch}: Loss={total_loss.item():.4f} (LR={current_lr:.1e})"
                for k, v in loss_components.items():
                    log_str += f" | {k}={v:.4f}"
                # ä½¿ç”¨ log_and_print æ›¿ä»£ print
                log_and_print(f"       {log_str}")

        # è¿”å›æœ€ç»ˆç»“æœæ—¶ï¼Œä¹Ÿè¦åº”ç”¨ä¸€æ¬¡çº¦æŸï¼Œç¡®ä¿è¾“å‡ºçš„æ˜¯ç‰©ç†åˆæ³•å€¼
        final_geometry = self._get_constrained_geometry(geometry_tensor).detach().cpu().numpy()
        q_matrix = q_opt.detach().cpu().numpy()

        return final_geometry, q_matrix, "\n".join(execution_log)

    # =========================================================================
    # âœ¨ æ–°å¢: [IDOF æ£€æµ‹ç‰ˆ] è¿åŠ¨å¯æŒç»­æ€§æ£€æŸ¥ Loss
    # =========================================================================
    def _loss_instantaneous_check(self, tensor, task, cycles=None, q_opt=None):
        """
        æ£€æŸ¥æœ«ç«¯è¿åŠ¨æ˜¯å¦å¯æŒç»­ï¼ˆåˆ¤æ–­æ˜¯å¦ä¸ºç¬æ—¶æœºæ„ï¼‰ã€‚
        åŸç†ï¼šè®¡ç®—äºŒé˜¶æ¼‚ç§» (Drift) æ˜¯å¦è½åœ¨é›…å¯æ¯”çŸ©é˜µçš„åˆ—ç©ºé—´å†…ã€‚
        """
        targets = task.get('targets', {})
        path_to_ee = targets.get('target_path_sequence', [])
        target_twists = targets.get('target_motion_twists', [])
        target_masks = targets.get('target_masks', [])

        # å¦‚æœæ²¡æœ‰å®šä¹‰è·¯å¾„æˆ–ç›®æ ‡èºæ—‹ï¼Œæ— æ³•æ‰§è¡Œæ­¤æ£€æŸ¥ï¼Œè¿”å› 0 loss
        if not path_to_ee or not target_twists:
            return torch.tensor(0.0, device=self.device)

        dt = 1e-3
        total_loss = torch.tensor(0.0, device=self.device)

        # å¤„ç†å¤šä¸ªç›®æ ‡ Twist (å¦‚æœæœ‰)
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šé€šå¸¸åªæœ‰ä¸€ä¸ª pathï¼Œä½†å¯èƒ½æœ‰å¤šä¸ª twist ç›®æ ‡ (å¤šæ¨¡å¼)
        # æˆ‘ä»¬å–ç¬¬ä¸€ä¸ª twist è¿›è¡Œæ£€æŸ¥ï¼Œæˆ–è€…éå†æ£€æŸ¥
        num_modes = len(target_twists)

        for k in range(num_modes):
            tgt_twist = torch.tensor(target_twists[k], device=self.device)
            # å¦‚æœæœ‰ mask å°±ç”¨ï¼Œæ²¡æœ‰å°±å…¨ 1
            if target_masks and k < len(target_masks):
                tgt_mask = torch.tensor(target_masks[k], device=self.device)
            else:
                tgt_mask = torch.ones(6, device=self.device)

            # --- æ­¥éª¤ 1: è·å–å½“å‰æ„å‹çš„ K å’Œ x0 ---
            # è¿™é‡Œçš„ K_curr å®é™…ä¸Šå°±æ˜¯åŒ…å«äº†"è™šæ‹Ÿç¯è·¯çº¦æŸ"ï¼ˆä»»åŠ¡çº¦æŸï¼‰çš„é›…å¯æ¯”çŸ©é˜µ
            mapping, K_curr, x0, spectrum = self._solve_anchor_system(
                tensor, q_opt, cycles,
                extended_task_path=path_to_ee,
                target_twist=tgt_twist,
                target_mask=tgt_mask,
                return_full_data=True  # âœ¨ è¯·æ±‚è¿”å›å®Œæ•´æ•°æ®
            )

            # åŸºç¡€æ£€æŸ¥ï¼šå¦‚æœæ˜¯æ­»é”æˆ–å½“å‰ä½ç½®å°±ä¸é—­åˆï¼Œç›´æ¥è·³è¿‡ (ç”±å…¶ä»– Loss è´Ÿè´£)
            if x0 is None or K_curr is None:
                continue

            # å½’ä¸€åŒ– x0 (å•ä½é€Ÿåº¦ï¼Œæ¶ˆé™¤é€Ÿåº¦å¤§å°å¯¹ Drift å¹…åº¦çš„å½±å“)
            x_norm = torch.norm(x0)
            if x_norm < 1e-6:
                continue
            x0 = x0 / x_norm

            # --- æ­¥éª¤ 2: è®¡ç®—äºŒé˜¶æ¼‚ç§» (The "Bill") ---
            # æˆ‘ä»¬ä½¿ç”¨æœ‰é™å·®åˆ†æ¥é€šè¿‡ PyTorch è‡ªåŠ¨è®¡ç®— (J_dot * q_dot)

            # 2.1 æ¨¡æ‹Ÿå‘å‰èµ°æå°çš„ä¸€æ­¥ q_next = q_current + x0 * dt
            q_next = q_opt.clone()

            # åˆ©ç”¨ mapping å°† x0 (åˆ—å‘é‡) æ˜ å°„å› q (çŸ©é˜µ)
            # mapping: {(u, v): col_idx}
            for (u, v), col_idx in mapping.items():
                if col_idx < len(x0):
                    val = x0[col_idx] * dt
                    q_next[u, v] = q_next[u, v] + val
                    q_next[v, u] = q_next[v, u] - val  # åå‘è¾¹å–å (å¦‚æœæ˜¯ R å…³èŠ‚)

            # 2.2 è·å–æ–°ä½ç½®çš„ K (æ— éœ€è§£æ–¹ç¨‹ï¼Œåªè¦çŸ©é˜µ)
            _, K_next, _, _ = self._solve_anchor_system(
                tensor, q_next, cycles,
                extended_task_path=path_to_ee,
                target_twist=tgt_twist,
                target_mask=tgt_mask,
                return_full_data=True
            )

            if K_next is None:
                total_loss = total_loss + 10.0
                continue

            # 2.3 è®¡ç®—æ¼‚ç§»å‘é‡ Drift = (K_next - K_curr) * x0 / dt
            # ç‰©ç†å«ä¹‰ï¼šä¿æŒå…³èŠ‚é€Ÿåº¦ä¸å˜æ—¶ï¼Œçº¦æŸæ–¹ç¨‹äº§ç”Ÿçš„ç ´åé€Ÿåº¦
            drift_vec = (K_next @ x0 - K_curr @ x0) / dt

            # --- æ­¥éª¤ 3: æŠ•å½±ç›¸å®¹æ€§æµ‹è¯• (The "Payment") ---
            # æ£€æŸ¥æ–¹ç¨‹ K_curr * alpha = -drift æ˜¯å¦æœ‰è§£

            # ä½¿ç”¨ä¼ªé€†è¿›è¡ŒæŠ•å½±: Residual = (I - K * K_pinv) * drift
            # rcond=1e-3 ç”¨äºå¿½ç•¥æå°çš„å¥‡å¼‚å€¼å™ªå£°
            try:
                # ä¸º pinv æ·»åŠ æŠ–åŠ¨ä¿æŠ¤
                if K_curr.requires_grad:
                    jitter = torch.randn_like(K_curr) * 1e-9
                    K_curr_noisy = K_curr + jitter
                else:
                    K_curr_noisy = K_curr

                K_pinv = torch.linalg.pinv(K_curr_noisy, rcond=1e-3)
                alpha_sol = K_pinv @ (-drift_vec)
                compensated_drift = K_curr @ alpha_sol

                residual_vec = (-drift_vec) - compensated_drift
                loss_idof = torch.norm(residual_vec)

                # æ ‡å‡†åŒ–ï¼šé™¤ä»¥ drift çš„æ¨¡é•¿ (Ratio)
                drift_norm = torch.norm(drift_vec)
                if drift_norm > 1e-6:
                    loss_ratio = loss_idof / drift_norm
                else:
                    loss_ratio = torch.tensor(0.0, device=self.device)  # å‡ ä¹æ²¡æœ‰æ¼‚ç§»ï¼Œå®Œç¾

                total_loss = total_loss + loss_ratio
            except:
                total_loss = total_loss + 1.0  # SVD å¤±è´¥æƒ©ç½š

        return total_loss / num_modes

    # =========================================================================
    # Helper: å¯å¾®ç‰©ç†çº¦æŸå±‚ (Differentiable Physics Constraints)
    # =========================================================================
    def _get_constrained_geometry(self, raw_tensor):
        """
        è¾“å…¥: åŒ…å«ä»»æ„å®æ•°å€¼çš„åŸå§‹å¼ é‡ (Raw Parameters)
        è¾“å‡º: ç¬¦åˆç‰©ç†çº¦æŸçš„å¼ é‡ (Physical Parameters)
        ç‰¹æ€§: å…¨ç¨‹å¯å¯¼ï¼Œæ¢¯åº¦å¯å›ä¼ 
        """
        # 1. æ‹†åˆ†é€šé“ (ä¸ºäº†ä¿æŒæ¢¯åº¦ï¼Œä¸è¦ä½¿ç”¨ detach)
        # tensor shape: (5, N, N)
        exists = raw_tensor[0]
        j_type = raw_tensor[1]
        a = raw_tensor[2]
        alpha = raw_tensor[3]
        offset = raw_tensor[4]

        # 2. å¼ºåˆ¶å¯¹ç§°æ€§ (Symmetry)
        # è¿æ†å±æ€§ a, alpha, exists å¿…é¡»æ˜¯å¯¹ç§°çŸ©é˜µ
        # æ“ä½œ: M_sym = (M + M.T) / 2
        # æ¢¯åº¦ä¼šå¹³å‡åˆ†é…ç»™ M_ij å’Œ M_ji
        exists_sym = (exists + exists.T) / 2.0
        a_sym = (a + a.T) / 2.0
        alpha_sym = (alpha + alpha.T) / 2.0

        # 3. å…³èŠ‚ç±»å‹è¡Œä¸€è‡´æ€§ (Row Consistency)
        # èŠ‚ç‚¹ç±»å‹ç”±è¡Œå†³å®šï¼Œå–è¡Œå‡å€¼å¹¶å¹¿æ’­
        j_type_row = j_type.mean(dim=1, keepdim=True).expand_as(j_type)

        # 4. ç‰©ç†åˆæ³•æ€§ (Positivity & Periodicity)
        # æ†é•¿ a å¿…é¡»éè´Ÿ -> ä½¿ç”¨ abs()
        a_phys = torch.abs(a_sym)

        # æ‰­è½¬è§’ alpha å‘¨æœŸæ€§ -> ä½¿ç”¨ remainder
        # æ³¨æ„: è¿™é‡Œçš„æ¢¯åº¦æ˜¯ 1 (çº¿æ€§)ï¼Œä¸ä¼šé˜»æ–­
        alpha_phys = torch.remainder(alpha_sym, 2 * math.pi)

        # offset å¯ä»¥ä¸ºè´Ÿï¼Œä¸åšé™åˆ¶
        offset_phys = offset

        # 5. æ‹“æ‰‘æ©ç  (Masking)
        # å¼ºåˆ¶éè¿æ¥å¤„çš„å‚æ•°ä¸º 0
        # ä½¿ç”¨åˆå§‹æ‹“æ‰‘ä½œä¸ºç¡¬ mask (å‡è®¾ Step 4 ä¸æ”¹å˜æ‹“æ‰‘)
        # è¿™é‡Œä½¿ç”¨ Sigmoid è¿‘ä¼¼ step function? ä¸ï¼Œç›´æ¥ç”¨ exists > 0.5 çš„ç¡¬ mask å³å¯
        # å› ä¸ºæˆ‘ä»¬ä¸æƒ³ä¼˜åŒ–ä¸å­˜åœ¨çš„è¾¹
        mask = (exists_sym > 0.5).float()

        a_final = a_phys * mask
        alpha_final = alpha_phys * mask
        offset_final = offset_phys * mask
        exists_final = exists_sym * mask  # å®é™…ä¸Šè¿™ä¼šæŠŠ exists å˜æˆ 0/1 (å¦‚æœå®ƒæ˜¯parameterçš„è¯)

        # 6. å¯¹è§’çº¿æ¸…é›¶ (No Self-loops)
        N = raw_tensor.shape[1]
        diag_mask = 1.0 - torch.eye(N, device=self.device)

        a_final = a_final * diag_mask
        alpha_final = alpha_final * diag_mask
        offset_final = offset_final * diag_mask
        exists_final = exists_final * diag_mask

        # 7. é‡æ–°å †å 
        return torch.stack([exists_final, j_type_row, a_final, alpha_final, offset_final], dim=0)

    # =========================================================================
    # Loss Functions
    # =========================================================================
    def _loss_closure(self, tensor, task, cycles=None, q_opt=None):
        """
        é—­ç¯è¯¯å·®è®¡ç®— (åŸºäºèŠ‚ç‚¹å¤šé‡è§‚æµ‹ä¸€è‡´æ€§)
        """
        # 1. å†…éƒ¨è°ƒç”¨æ ¸å¿ƒå¼•æ“ï¼Œè·å–æ‰€æœ‰èŠ‚ç‚¹çš„è§‚æµ‹çŠ¶æ€
        # å‡è®¾ base_node ä¸º 0ï¼Œæˆ–è€…æ ¹æ® cycles è‡ªåŠ¨æ¨æ–­
        base_node = 0
        if cycles and len(cycles) > 0:
            base_node = cycles[0][0]  # å°è¯•ä½¿ç”¨ç¯è·¯ä¸­çš„ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä½œä¸º base

        node_observations = self._compute_multi_path_states(tensor, q_opt, base_node=base_node)

        # for key, value in node_observations.items():
        #     print(key, value)

        total_loss = torch.tensor(0.0, device=self.device)
        count = 0

        # 2. éå†è§‚æµ‹ç»“æœï¼Œè®¡ç®—æ–¹å·®/åå·®
        for node_id, obs_list in node_observations.items():
            # å¦‚æœåªæœ‰ 1 ä¸ªè§‚æµ‹å€¼ï¼Œè¯´æ˜æ²¡æœ‰é—­ç¯å†²çªï¼Œè·³è¿‡
            if len(obs_list) < 2:
                continue

            # ä»¥ç¬¬ä¸€ä¸ªè§‚æµ‹å€¼ä¸ºåŸºå‡† (Anchor)
            ref_P = obs_list[0]['P']
            ref_z = obs_list[0]['z']

            # å¼ºåˆ¶æ‰€æœ‰åç»­è§‚æµ‹å€¼ä¸åŸºå‡†ä¸€è‡´
            for i in range(1, len(obs_list)):
                curr_P = obs_list[i]['P']
                curr_z = obs_list[i]['z']

                loss_pos = torch.sum((curr_P - ref_P) ** 2)
                loss_align = torch.sum((curr_z - ref_z) ** 2)

                total_loss = total_loss + loss_pos + loss_align
                count += 1

        return total_loss

    def _loss_mobility_dof(self, tensor, task, cycles=None, q_opt=None):
        device = self.device

        # === æ ¸å¿ƒä¿®å¤ï¼šè®¡ç®—å®é™…æ´»è·ƒèŠ‚ç‚¹æ•°ï¼Œè€Œé Tensor ç»´åº¦ ===
        # åŸä»£ç : num_nodes = tensor.shape[1]
        if cycles:
            # ç»Ÿè®¡æ‰€æœ‰åœ¨é—­ç¯(cycles)ä¸­å‡ºç°çš„å”¯ä¸€èŠ‚ç‚¹
            active_nodes = set()
            for cycle in cycles:
                active_nodes.update(cycle)
            num_nodes = len(active_nodes)
        else:
            # å¦‚æœæ²¡æœ‰é—­ç¯ä¿¡æ¯ï¼ˆæç«¯æƒ…å†µï¼‰ï¼Œå›é€€åˆ° Tensor ç»´åº¦ï¼Œæˆ–è€…ç›´æ¥è¿”å›
            num_nodes = tensor.shape[1]

        try:
            target_dof = task.get('kinematics', {}).get('dof', 1)
        except:
            target_dof = 1

        _, _, _, spectrum = self._solve_anchor_system(
            tensor, q_opt, cycles,
            extended_task_path=None, target_twist=None, target_mask=None,
            return_spectrum=True
        )

        if spectrum is None: return torch.tensor(10.0, device=device)

        target_zero_count = num_nodes + target_dof

        if len(spectrum) <= target_zero_count:
            # å¦‚æœè°±çš„é•¿åº¦æ¯”ç›®æ ‡è¿˜çŸ­ï¼Œè¯´æ˜çŸ©é˜µå¤ªå°äº†ï¼Œå¯èƒ½è¿˜æ²¡æœ‰å½¢æˆæœ‰æ•ˆçº¦æŸ
            # æˆ–è€…è¿™é‡ŒåŸæœ¬çš„é€»è¾‘æ˜¯æƒ³è¦æƒ©ç½šâ€œéé›¶â€ç‰¹å¾å€¼
            # è¿™ç§æƒ…å†µä¸‹è¿”å› 1.0 å¯èƒ½æ˜¯ä¸åˆé€‚çš„ï¼Œè§†å…·ä½“æ•°å­¦æ¨å¯¼è€Œå®š
            # ä½†æ—¢ç„¶ä¹‹å‰æ˜¯å› ä¸º 8 > 4 å¯¼è‡´çš„è¯¯åˆ¤ï¼Œç°åœ¨ num_nodes ä¿®æ­£ä¸º 4 åï¼Œ
            # target_zero_count å˜å°ï¼Œè¿™ä¸ª if æ¡ä»¶å°±ä¸å®¹æ˜“è¯¯è§¦å‘äº†ã€‚
            return torch.tensor(1.0, device=device)

        zeros_part = spectrum[:target_zero_count]
        loss_zeros = torch.sum(zeros_part ** 2) * 10.0
        return loss_zeros

    def _loss_path_generation(self, tensor, task, cycles=None, q_opt=None):
        targets = task.get('targets', {})
        target_path_seq = targets.get('target_path_sequence', [])
        target_poses = targets.get('target_motion_twists', [])

        if not target_path_seq or not target_poses:
            return torch.tensor(0.0, device=self.device)

        # FK Path: Base -> ... -> EE (å»é™¤ Ghost)
        fk_path_nodes = target_path_seq[1:-1]
        ghost_in = target_path_seq[0]

        T_ee = torch.eye(4, device=self.device)
        for i in range(len(fk_path_nodes) - 1):
            curr = fk_path_nodes[i]
            next_n = fk_path_nodes[i + 1]
            prev = ghost_in if i == 0 else fk_path_nodes[i - 1]

            j_type = tensor[1, curr, next_n]
            a = torch.abs(tensor[2, curr, next_n])
            alpha = tensor[3, curr, next_n]

            off_out = tensor[4, curr, next_n]
            off_in = tensor[4, curr, prev]
            d_static = off_out - off_in

            q_out = q_opt[curr, next_n]
            q_in = q_opt[curr, prev]
            q_diff = q_out - q_in

            is_R = (j_type > 0.5).float()
            is_P = 1.0 - is_R
            theta = is_R * (q_diff - PI) + is_P * (d_static - PI)
            d = is_R * d_static + is_P * q_diff

            T_step = self._get_dh_matrix_fast(a, alpha, d, theta)
            T_ee = T_ee @ T_step

        tgt_vec = torch.tensor(target_poses[0], device=self.device)
        T_target = self._vec6_to_matrix(tgt_vec)

        pos_err = torch.norm(T_ee[:3, 3] - T_target[:3, 3])
        rot_err = torch.norm(T_ee[:3, :3] - T_target[:3, :3])

        return pos_err + rot_err

    def _loss_twist_alignment(self, tensor, task, cycles=None, q_opt=None):
        targets = task.get('targets', {})
        target_path_seq = targets.get('target_path_sequence', [])
        target_twists = targets.get('target_motion_twists', [])
        target_masks = targets.get('target_masks', [])

        if not target_path_seq or not target_twists:
            return torch.tensor(0.0, device=self.device)

        tgt_twist = torch.tensor(target_twists[0], device=self.device)
        tgt_mask = torch.tensor(target_masks[0], device=self.device) if target_masks else torch.ones(6,
                                                                                                     device=self.device)

        _, _, _, spectrum = self._solve_anchor_system(
            tensor, q_opt, cycles,
            extended_task_path=target_path_seq,
            target_twist=tgt_twist,
            target_mask=tgt_mask,
            return_spectrum=True
        )

        if spectrum is None: return torch.tensor(10.0, device=self.device)
        num_nodes = tensor.shape[1]
        if len(spectrum) > num_nodes:
            return torch.abs(spectrum[num_nodes]) * 50.0
        return torch.tensor(0.0, device=self.device)

    def _loss_bennett_condition(self, tensor, task, cycles=None, q_opt=None):
        if not cycles: return torch.tensor(0.0, device=self.device)
        total_error = torch.tensor(0.0, device=self.device)
        TWO_PI = 2 * math.pi
        for path in cycles:
            if len(path) != 4: continue
            a_list = []
            alpha_list = []
            offset_loss_accum = torch.tensor(0.0, device=self.device)
            max_a = torch.tensor(1.0, device=self.device)
            L = 4
            for i in range(L):
                curr, next_n, prev = path[i], path[(i + 1) % L], path[(i - 1 + L) % L]
                a = torch.abs(tensor[2, curr, next_n])
                alpha = tensor[3, curr, next_n] % TWO_PI
                max_a = torch.max(max_a, a)
                a_list.append(a)
                alpha_list.append(alpha)
                off_out = tensor[4, curr, next_n]
                off_in = tensor[4, curr, prev]
                d_val = off_out - off_in
                offset_loss_accum += d_val ** 2
            offset_loss_rel = offset_loss_accum / (max_a ** 2 + 1e-6)
            a_vec = torch.stack(a_list)
            alpha_vec = torch.stack(alpha_list)
            sym_loss_a = ((a_vec[0] - a_vec[2]) ** 2) / (a_vec[0] ** 2 + a_vec[2] ** 2 + 1e-6) + \
                         ((a_vec[1] - a_vec[3]) ** 2) / (a_vec[1] ** 2 + a_vec[3] ** 2 + 1e-6)
            # ä¿®æ”¹å (å¯¹å‘¨æœŸæ€§ä¸æ•æ„Ÿ)
            # 1 - cos(diff) åœ¨ diff=0 æ—¶ä¸º 0ï¼Œåœ¨ diff=2pi æ—¶ä¹Ÿä¸º 0ï¼Œå®Œç¾è§£å†³æ–­å±‚
            sym_loss_alpha = (1.0 - torch.cos(alpha_vec[0] - alpha_vec[2])) + \
                             (1.0 - torch.cos(alpha_vec[1] - alpha_vec[3]))
            sin_alpha = torch.sin(alpha_vec)
            term1 = a_vec[0] * sin_alpha[1]
            term2 = a_vec[1] * sin_alpha[0]
            ratio_err1 = (term1 - term2) ** 2 / (term1 ** 2 + term2 ** 2 + 1e-6)
            term3 = a_vec[1] * sin_alpha[2]
            term4 = a_vec[2] * sin_alpha[1]
            ratio_err2 = (term3 - term4) ** 2 / (term3 ** 2 + term4 ** 2 + 1e-6)
            total_error += sym_loss_a + sym_loss_alpha + ratio_err1 + ratio_err2 + offset_loss_rel * 5.0
        # print(f"  > Symmetry A: {sym_loss_a.item():.6f}")
        # print(f"  > Symmetry Alpha: {sym_loss_alpha.item():.6f}")
        # print(f"  > Ratio Error: {ratio_err2.item():.6f}")
        # print(f"  > Offset Error (d=0): {(offset_loss_rel * 5.0).item():.6f}")  # <--- é‡ç‚¹å…³æ³¨è¿™ä¸ª

        return total_error

    # =========================================================================
    # Helpers
    # =========================================================================
    def _compute_all_joint_screws(self, structure, q_current, base_node, cycles=None, normalize=True):
        """
        è®¡ç®—æ‰€æœ‰å…³èŠ‚èºæ—‹ (Screws)ã€‚
        å†…éƒ¨è°ƒç”¨å¤šè·¯å¾„è§£ç®—å™¨ï¼Œæå–ç”Ÿæˆæ ‘çŠ¶æ€ï¼Œå¹¶è¿›è¡Œç‰¹å¾é•¿åº¦å½’ä¸€åŒ–ã€‚
        """
        # 1. å†…éƒ¨è°ƒç”¨æ ¸å¿ƒå¼•æ“
        node_observations = self._compute_multi_path_states(structure, q_current, base_node=base_node)

        N = structure.shape[1]
        screws = torch.zeros((N, 6), device=self.device)

        # 2. è®¡ç®—ç‰¹å¾é•¿åº¦ L_char (ç”¨äºå½’ä¸€åŒ–)
        L_char = torch.tensor(1.0, device=self.device)
        if normalize:
            exists_mask = structure[0] > 0.5
            all_a = torch.abs(structure[2][exists_mask])
            valid_a = all_a[all_a > 1e-6]
            if valid_a.numel() > 0:
                L_char = torch.mean(valid_a)

        # 3. æå–èºæ—‹
        for u in range(N):
            # å¦‚æœèŠ‚ç‚¹æœªè¿é€šï¼Œä¿æŒ 0
            if u not in node_observations:
                continue

            # æ€»æ˜¯å–åˆ—è¡¨ä¸­çš„ç¬¬ 0 ä¸ªè§‚æµ‹å€¼ä½œä¸º"æƒå¨çŠ¶æ€"
            # (Jacobian è®¡ç®—åªéœ€è¦ä¸€å¥—è‡ªæ´½çš„åæ ‡ç³»)
            state = node_observations[u][0]
            P = state['P']
            z = state['z']

            row_types = structure[1, u, :]
            is_R = not (row_types < -0.5).any()

            if is_R:
                w = z
                v = torch.linalg.cross(P, z)

                # [å½’ä¸€åŒ–] ä»…ç¼©æ”¾åŠ›çŸ©éƒ¨åˆ†ï¼Œä½¿æ—‹è½¬å’Œç§»åŠ¨é‡çº§åŒ¹é…
                if normalize:
                    v = v / L_char

                screws[u] = torch.cat([w, v])
            else:
                # På‰¯: w=0, v=z (ç§»åŠ¨æ–¹å‘)
                w = torch.zeros(3, device=self.device)
                v = z
                screws[u] = torch.cat([w, v])

        return screws

    def _solve_anchor_system(self, structure, q_current, loops, extended_task_path=None, target_twist=None,
                             target_mask=None, return_spectrum=False, return_full_data=False):
        # 1. ç¡®å®šè®¡ç®—æ‰€éœ€çš„èŠ‚ç‚¹å’Œ Screw
        # ------------------------------------------------------------------
        # æ”¶é›†æ‰€æœ‰æ´»è·ƒèŠ‚ç‚¹ä»¥ç¡®å®š base_node
        active_nodes = set()
        for loop in loops: active_nodes.update(loop)
        if extended_task_path: active_nodes.update([n for n in extended_task_path if n >= 0])

        # å¦‚æœæ²¡æœ‰æ´»è·ƒèŠ‚ç‚¹ï¼Œç›´æ¥è¿”å›
        if not active_nodes: return None, None, None, None

        base_node = extended_task_path[1] if extended_task_path and len(extended_task_path) > 1 else min(active_nodes)
        # è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„èºæ—‹è½´ (Joint Screws)
        all_screws = self._compute_all_joint_screws(
            structure, q_current, base_node,
            cycles=loops,
            normalize=True  # <--- ç¡®ä¿è¿™é‡Œå¼€å¯
        )
        num_nodes = structure.shape[1]

        # 2. å»ºç«‹å˜é‡æ˜ å°„ (Edge to Column Mapping) - å‚è€ƒæä¾›çš„ NumPy é€»è¾‘
        # ------------------------------------------------------------------
        # æ”¶é›†æ‰€æœ‰æ¶‰åŠçš„æ— å‘è¾¹ (u, v) å…¶ä¸­ u < v
        involved_edges_set = set()

        # A. ä»å›è·¯ä¸­æ”¶é›†è¾¹
        for loop in loops:
            L = len(loop)
            for i in range(L):
                u, v = loop[i], loop[(i + 1) % L]
                involved_edges_set.add(tuple(sorted((u, v))))

        # B. ä»ä»»åŠ¡è·¯å¾„ä¸­æ”¶é›†è¾¹
        if extended_task_path:
            for i in range(len(extended_task_path) - 1):
                u = extended_task_path[i]
                v = extended_task_path[i + 1]
                if u >= 0 and v >= 0:  # ç¡®ä¿èŠ‚ç‚¹ç´¢å¼•æœ‰æ•ˆ
                    involved_edges_set.add(tuple(sorted((u, v))))

        # C. æ„å»ºæœ‰å‘è¾¹æ˜ å°„ (u->v å’Œ v->u å¯¹åº”ä¸åŒçš„åˆ—)
        edge_to_col = {}
        current_col = 0
        for u, v in involved_edges_set:
            # æ·»åŠ  (u, v)
            edge_to_col[(u, v)] = current_col
            current_col += 1
            # æ·»åŠ  (v, u)
            edge_to_col[(v, u)] = current_col
            current_col += 1

        num_vars_reduced = current_col  # å®é™…çš„å˜é‡æ•°é‡ (æ— éœ€å†ç”¨ mask è¿‡æ»¤)

        if num_vars_reduced == 0: return None, None, None, None

        # 3. æ„å»º Jacobian çŸ©é˜µ K
        # ------------------------------------------------------------------
        num_loops = len(loops)
        has_task = (target_twist is not None)
        total_rows = 6 * (num_loops + (1 if has_task else 0))

        # ç›´æ¥å»ºç«‹ç´§å‡‘çŸ©é˜µï¼Œä¸å†å»ºç«‹ num_nodes*num_nodes çš„å¤§çŸ©é˜µ
        K_compact = torch.zeros((total_rows, num_vars_reduced), device=self.device)
        b = torch.zeros(total_rows, device=self.device)

        current_row = 0

        # --- å¡«å……å›è·¯çº¦æŸ ---
        for loop in loops:
            L = len(loop)
            for i in range(L):
                curr, next_n, prev = loop[i], loop[(i + 1) % L], loop[(i - 1 + L) % L]
                screw = all_screws[curr]

                # å¯¹åº” NumPy ä»£ç : K_local[..., edge_to_col[(curr, next)]] += screw
                if (curr, next_n) in edge_to_col:
                    col_idx = edge_to_col[(curr, next_n)]
                    K_compact[current_row:current_row + 6, col_idx] += screw

                # å¯¹åº” NumPy ä»£ç : K_local[..., edge_to_col[(curr, prev)]] -= screw
                if (curr, prev) in edge_to_col:
                    col_idx = edge_to_col[(curr, prev)]
                    K_compact[current_row:current_row + 6, col_idx] -= screw

            current_row += 6

        # --- å¡«å……ä»»åŠ¡/è·¯å¾„çº¦æŸ ---
        if has_task and extended_task_path:
            # åŒæ ·ä½¿ç”¨ç´§å‡‘çŸ©é˜µçš„ä¸€è¡Œ
            row_slice = slice(current_row, current_row + 6)

            for i in range(1, len(extended_task_path) - 1):
                curr = extended_task_path[i]
                prev_n = extended_task_path[i - 1]
                next_n = extended_task_path[i + 1]
                screw = all_screws[curr]

                if next_n is not None and next_n >= 0:
                    if (curr, next_n) in edge_to_col:
                        col_idx = edge_to_col[(curr, next_n)]
                        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å¤„ç† target_maskï¼Œç¨åç»Ÿä¸€å¤„ç†æˆ–åœ¨æ­¤å¤„å¤„ç†
                        # ä¸ºä¿æŒé€»è¾‘æ¸…æ™°ï¼Œå…ˆå¡«å…¥ K_compactï¼Œæœ€åå†ä¹˜ mask
                        K_compact[row_slice, col_idx] += screw

                if prev_n is not None and prev_n >= 0:
                    if (curr, prev_n) in edge_to_col:
                        col_idx = edge_to_col[(curr, prev_n)]
                        K_compact[row_slice, col_idx] -= screw

            # åº”ç”¨ Task Mask å’Œ Target Twist
            if target_mask is not None:
                # K_compact çš„å¯¹åº”è¡Œä¹˜ä»¥ mask (unsqueezeç”¨äºå¹¿æ’­)
                K_compact[row_slice, :] *= target_mask.unsqueeze(1)
                b[row_slice] = target_twist * target_mask
            else:
                b[row_slice] = target_twist

        # --- 4. SVD åˆ†è§£ä¸æ•°æ®è¿”å› (ä¿®æ”¹éƒ¨åˆ†) ---
        b_unsqueezed = b.unsqueeze(1)
        K_aug = torch.cat([K_compact, -b_unsqueezed], dim=1) if has_task else K_compact

        try:
            # æ·»åŠ å¾®å°æŠ–åŠ¨ä»¥é˜²æ­¢ SVD æ¢¯åº¦çˆ†ç‚¸
            if getattr(self.cfg, 'enable_jitter', True) and K_aug.requires_grad:
                jitter = torch.randn_like(K_aug) * 1e-9
                K_aug = K_aug + jitter

            # Perform SVD
            U, S, Vh = torch.linalg.svd(K_aug, full_matrices=True)

            # S è¡¥é½é€»è¾‘ (ä¿æŒä¸å˜)
            num_vars_aug = K_aug.shape[1]
            full_S = torch.zeros(num_vars_aug, dtype=S.dtype, device=self.device)
            full_S[:S.shape[0]] = S
            spectrum = torch.flip(full_S, dims=[0])

            # âœ¨ æ–°å¢: å¦‚æœè¯·æ±‚å®Œæ•´æ•°æ®ï¼Œè®¡ç®— x0 å¹¶è¿”å›
            if return_full_data:
                # x0 æ˜¯ K_aug çš„é›¶ç©ºé—´å‘é‡ï¼Œå¯¹åº”æœ€å°å¥‡å¼‚å€¼çš„å³å¥‡å¼‚å‘é‡ (Vh çš„æœ€åä¸€è¡Œ)
                # Vh æ˜¯ (min(M, N), N)ï¼ŒSVD ä¹Ÿæ˜¯æŒ‰å¥‡å¼‚å€¼é™åºæ’åˆ—çš„
                # æ‰€ä»¥ Vh[-1] å¯¹åº”æœ€å°å¥‡å¼‚å€¼
                x0 = Vh[-1, :]
                return edge_to_col, K_aug, x0, spectrum

            return None, None, None, spectrum
        except Exception as e:
            return None, None, None, None

    def _get_dh_matrix_fast(self, a, alpha, d, theta):
        """
        æ„å»ºæ ‡å‡† DH å˜æ¢çŸ©é˜µ (Standard DH)
        æ”¯æŒå¹¿æ’­: è¾“å…¥å¯ä»¥æ˜¯æ ‡é‡ï¼Œä¹Ÿå¯ä»¥æ˜¯ (Batch, N, N) ç­‰ä»»æ„ç»´åº¦
        è¾“å‡º: (..., 4, 4)
        """
        # 1. é¢„è®¡ç®—ä¸‰è§’å‡½æ•°
        ct, st = torch.cos(theta), torch.sin(theta)
        ca, sa = torch.cos(alpha), torch.sin(alpha)

        # 2. å‡†å¤‡å ä½ç¬¦ (0 å’Œ 1)ï¼Œå½¢çŠ¶ä¸è¾“å…¥ä¸€è‡´ï¼Œä¿æŒ device/dtype æ­£ç¡®
        zero = torch.zeros_like(theta)
        one = torch.ones_like(theta)

        # 3. é€ä¸ªå…ƒç´ æ„å»º (è¡Œä¼˜å…ˆ)
        # è¿™é‡Œçš„å…³é”®æ˜¯ï¼šä¸ä½¿ç”¨ stack å †å æˆåˆ—è¡¨ï¼Œè€Œæ˜¯ç›´æ¥æ„é€ æœ€åä¸€ç»´

        # Row 1: [ct, -st*ca, st*sa, a*ct]
        r11, r12, r13, r14 = ct, -st * ca, st * sa, a * ct

        # Row 2: [st, ct*ca, -ct*sa, a*st]
        r21, r22, r23, r24 = st, ct * ca, -ct * sa, a * st

        # Row 3: [0, sa, ca, d]
        r31, r32, r33, r34 = zero, sa, ca, d

        # Row 4: [0, 0, 0, 1]
        r41, r42, r43, r44 = zero, zero, zero, one

        # 4. å †å æˆçŸ©é˜µ (..., 4, 4)
        # å…ˆå †å æˆè¡Œ (..., 4)ï¼Œå†å †å æˆçŸ©é˜µ
        row1 = torch.stack([r11, r12, r13, r14], dim=-1)
        row2 = torch.stack([r21, r22, r23, r24], dim=-1)
        row3 = torch.stack([r31, r32, r33, r34], dim=-1)
        row4 = torch.stack([r41, r42, r43, r44], dim=-1)

        # æœ€ç»ˆç»„åˆ
        T = torch.stack([row1, row2, row3, row4], dim=-2)

        return T

    def _find_fundamental_cycles(self, adj_matrix):
        rows, cols = np.where(adj_matrix > 0.5)
        G = nx.Graph()
        G.add_edges_from(zip(rows, cols))
        try:
            return [list(cycle) for cycle in nx.cycle_basis(G)]
        except:
            return []

    def _build_nx_graph(self, adj_matrix):
        rows, cols = np.where(adj_matrix > 0.5)
        G = nx.Graph()
        G.add_edges_from(zip(rows, cols))
        return G

    def _vec6_to_matrix(self, vec6):
        x, y, z, rx, ry, rz = vec6
        cx, sx = torch.cos(rx), torch.sin(rx)
        cy, sy = torch.cos(ry), torch.sin(ry)
        cz, sz = torch.cos(rz), torch.sin(rz)
        R = torch.stack([
            torch.stack([cy * cz, cz * sx * sy - cx * sz, cx * cz * sy + sx * sz]),
            torch.stack([cy * sz, cx * cz + sx * sy * sz, -cz * sx + cx * sy * sz]),
            torch.stack([-sy, cy * sx, cx * cy])
        ])
        T = torch.eye(4, device=self.device)
        T[:3, :3] = R
        T[:3, 3] = torch.stack([x, y, z])
        return T

    # =========================================================================
    # ç”Ÿæˆæ ‘è¿åŠ¨å­¦ (å¼•ç”¨é‚»å±…ä½œä¸º Base å‚è€ƒï¼ŒåŸºäº ID æŸ¥è¯¢å‚æ•°)
    # =========================================================================
    def _compute_multi_path_states(self, structure, q_current, base_node=0):
        """
        åŠŸèƒ½: å¤šè·¯å¾„è¿åŠ¨å­¦è§£ç®—å™¨ (ä¿®æ­£ç‰ˆ: å¤„ç†å…¥è¾¹åå‘å¼•èµ·çš„ 180 åº¦ç›¸ä½å·®)
        """
        N = structure.shape[1]
        node_observations = {}
        expanded_nodes = set()

        # 1. ç¡®å®š Base çš„å‚è€ƒèŠ‚ç‚¹
        base_neighbors = torch.nonzero(structure[0, base_node, :] > 0.5).view(-1).tolist()
        if not base_neighbors: return {}
        ref_node = base_neighbors[0]

        T_base = torch.eye(4, device=self.device)

        # --- Base åˆå§‹åŒ– ---
        node_observations[base_node] = []
        node_observations[base_node].append({
            'P': T_base[:3, 3],
            'z': T_base[:3, 2],
            'T': T_base,
            'parent': ref_node
        })

        # æ ‡è®° Base å·²æ‰©å±•
        expanded_nodes.add(base_node)

        # Stack: (current, parent, T_current)
        stack = [(base_node, -1, T_base)]

        # å®šä¹‰ PI å¸¸é‡
        PI = torch.tensor(math.pi, device=self.device)

        while stack:
            u, p, T_u = stack.pop()

            # --- å‡†å¤‡ u çš„å…¥å‚ ---
            # è¿™äº›å‚æ•°ä»£è¡¨ u->p (ç¦»å¼€ u) çš„ç»å¯¹ä½ç½®
            if p == -1:
                off_in = structure[4, u, ref_node]
                q_in = q_current[u, ref_node]
            else:
                off_in = structure[4, u, p]
                q_in = q_current[u, p]

            neighbors = torch.nonzero(structure[0, u, :] > 0.5).view(-1).tolist()

            for v in neighbors:
                # é€»è¾‘çˆ¶èŠ‚ç‚¹åˆ¤æ–­
                logical_parent = ref_node if p == -1 else p

                # ç¦æ­¢å›å¤´
                if v == logical_parent: continue

                # --- è®¡ç®— u -> v ---
                a = structure[2, u, v]
                alpha = structure[3, u, v]
                off_out = structure[4, u, v]
                q_out = q_current[u, v]

                # åŸå§‹å·®åˆ† (Out - Stored_In)
                delta_off = off_out - off_in
                delta_q = q_out - q_in

                row_types = structure[1, u, :]
                is_R = not (row_types < -0.5).any()

                # ğŸŒŸ æ ¸å¿ƒä¿®æ­£: è§’åº¦ theta éœ€è¦ +/- 180 åº¦ (PI)
                # å› ä¸º Stored_In æ˜¯ u->p, ä½†ç‰©ç† In æ˜¯ p->u (æ–¹å‘ç›¸å)
                if is_R:
                    # å¯¹äº R å‰¯, theta ç”± q å†³å®š
                    # theta = q_out - (q_in + PI) = delta_q - PI
                    # è¿™é‡ŒåŠ å‡ PI å¯¹ä¸‰è§’å‡½æ•°ç»“æœæ˜¯ä¸€æ ·çš„ (sin(x+pi) = -sin(x))
                    # æˆ‘ä»¬ç»Ÿä¸€å‡å» PI (æˆ–è€…åŠ ä¸Š PI)
                    theta = delta_q - PI
                    d = delta_off
                else:
                    # å¯¹äº P å‰¯, theta é€šå¸¸ç”± offset å†³å®š (å¦‚æœæ˜¯å®šä¹‰è§’åº¦çš„è¯)
                    # d ç”± q å†³å®š (æ²¿ç€ z è½´çš„è·ç¦», æ–¹å‘åè½¬é€šå¸¸ä¸å½±å“ z è½´æ ‡é‡é•¿åº¦, é™¤éåæ ‡ç³»å®šä¹‰å¯¼è‡´ z è½´åå‘)
                    # å‡è®¾ offset ä¹Ÿæ˜¯ç»å¯¹è§’åº¦:
                    theta = delta_off - PI
                    d = delta_q

                # æ„å»º DH çŸ©é˜µ
                T_step = self._get_dh_matrix_fast(a, alpha, d, theta)
                T_v = T_u @ T_step

                # --- è®°å½•è§‚æµ‹ ---
                if v not in node_observations:
                    node_observations[v] = []

                node_observations[v].append({
                    'P': T_v[:3, 3],
                    'z': T_v[:3, 2],
                    'T': T_v,
                    'parent': u
                })

                # --- é€’å½’æ§åˆ¶ ---
                if v not in expanded_nodes:
                    expanded_nodes.add(v)
                    stack.append((v, u, T_v))
                else:
                    pass

        return node_observations